---
layout: post
title: "[CS231n] 4. Introduction to Neural Networks"
date: 2026-01-15 09:50
categories: MyStudy CS231n
tags: cv CS231n
math: true
---

강의 주소: <br>
[CS231n Lecture 4. Introduction to Neural Networks](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4)

자료: <br>
[깃허브 강의 자료](https://github.com/visionNoob/CS231N_17_KOR_SUB?tab=readme-ov-file)



### 1. 배경 지식 복습: 분류기와 손실 함수
이전 강의들에서는 가중치 $W$와 데이터 $X$를 입력으로 받아 클래스 점수를 출력하는 함수 $f(X, W)$를 정의하는 법을 배웠다.

**Loss Function:**  
SVM 손실 함수와 같이 모델의 예측에 대해 얼마나 만족하는지를 수치화한다.

**Regularization:**  
모델의 복잡도를 조절하여 일반화 성능을 높인다.

**최적화(Optimization):**  
손실을 최소화하는 $W$를 찾기 위해 Gradient Descent을 사용하여 손실 경사면의 가장 가파른 방향으로 이동한다.

**그래디언트 계산:**  
수치적 그래디언트는 구현이 쉽지만 느리고 근사치인 반면 **해석적 그래디언트(Analytic Gradient)** 는 정확하고 빠르지만 유도가 어렵고 실수하기 쉽다. 따라서 실제로는 해석적 그래디언트를 사용하되 수치적 그래디언트로 검증한다.

### 2. Computational Graphs
복잡한 함수의 해석적 그래디언트를 구하기 위해 **계산 그래프** 프레임워크를 사용한다.

**정의:**  
임의의 함수를 그래프로 표현하며 **Node**는 계산 단계를 나타냅니다.


>선형 분류기의 경우 가중치 곱셈 힌지 손실 계산 규제화 항 계산 등이 각각의 노드가 되며 최종 손실 $L$은 이들의 합으로 표현된다.

**장점:**  
함수가 아무리 복잡하더라도(CNN이나 Neural Turing Machine 등) 계산 그래프로 표현하면 **Backpropagation** 기술을 사용하여 모든 변수에 대한 그래디언트를 재귀적으로 계산할 수 있다.

### 3. Backpropagation의 작동 원리
#### (1) 간단한 스칼라 예제 ($f(x y z) = (x + y)z$)
1.  **Forward Pass:**  
입력값($x=-2 y=5 z=-4$)을 사용하여 그래프를 따라 최종 출력값 $f=-12$를 구한다. 중간 변수를 $q = x + y$라고 정의한다.

2.  **Backward Pass:**  
출력에서 시작하여 뒤로 이동하며 **Chain Rule**을 적용한다.  
    최종 출력에 대한 그래디언트는 $\frac{df}{df} = 1$이다.  
    $\frac{df}{dz} = q = 3$.  
    $\frac{df}{dq} = z = -4$.  
    **연쇄 법칙 적용:**  
    $y$에 대한 그래디언트 $\frac{df}{dy}$를 구할 때 직접 연결되지 않았으므로 $\frac{df}{dq} \times \frac{dq}{dy}$를 계산한다. $\frac{dq}{dy} = 1$이므로 $\frac{df}{dy} = -4 \times 1 = -4$가 됩니다. $x$에 대해서도 동일하게 $\frac{df}{dx} = -4$

#### (2) 노드의 지역적 관점(Local Intuition)
각 노드는 오직 **자신의 직접적인 주변**만을 인식한다.

**Local Gradient:**  
노드로 들어오는 입력에 대한 출력의 미분값으로 전방향 패스 중에 미리 계산해 둘 수 있다.

**Upstream Gradient:**  
뒤에서부터 전달되어 온 최종 손실에 대한 해당 노드 출력의 미분값이다.
역전파 중에 노드는 **[업스트림 그래디언트] $\times$ [지역적 그래디언트]**를 수행하여 자신의 입력단으로 전달할 그래디언트를 계산한다.

### 4. 복잡한 예제: 시그모이드(Sigmoid) 함수
함수 $f(w x) = \frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}$를 계산 그래프로 풀어서 역전파를 수행한다.

그래프는 곱셈 덧셈 $x(-1)$ $\exp$ $+1$ $1/x$ 등의 아주 작은 단위 노드로 쪼개질 수 있다.

$1/x$ 노드: 지역적 그래디언트는 $-\frac{1}{x^2}$이다.  
$+1$ 노드: 상수를 더하는 노드는 그래디언트를 그대로 통과시킨다.  
$\exp$ 노드: 지역적 그래디언트는 $e^x$이다.

**노드의 세분화(Granularity):**  
원한다면 여러 노드를 하나의 **시그모이드 게이트**로 묶을 수 있다. 시그모이드 함수의 미분은 $\frac{d\sigma(x)}{dx} = (1 - \sigma(x))\sigma(x)$라는 깔끔한 형태를 가지므로 이를 활용하면 계산을 단순화하고 그래프를 간결하게 만들 수 있다.

### 5. 역전파의 패턴과 직관
**Add gate:**  
업스트림 그래디언트를 연결된 모든 입력으로 동일하게 복사해주는 **그래디언트 분배기** 역할을 한다.

**Max gate:**  
값이 더 큰 쪽으로만 그래디언트를 전달하고 다른 쪽은 0으로 만드는 **그래디언트 Router** 역할을 한다.

**Mul gate:**  
업스트림 그래디언트에 상대방 입력의 값을 곱해주는 **그래디언트 Switcher/Scaler** 역할을 한다.

**다중 연결 노드:**  
한 변수가 여러 곳에 연결된 경우 각 경로에서 돌아오는 **그래디언트들을 모두 합산**한다 (다변수 연쇄 법칙).

### 6. Vectorized Operations
입력이 스칼라가 아닌 벡터나 행렬일 경우에도 원리는 같다.

**야코비안 행렬(Jacobian Matrix):**  
각 입력 요소에 대한 각 출력 요소의 편미분을 담은 행렬이다.

입력이 4096차원인 경우 야코비안은 $4096 \times 4096$ 크기가 되어 매우 크지만 요소별 연산(예: ReLU)의 경우 야코비안은 **Diagonal Matrix** 형태가 되므로 실제로는 전체 행렬을 계산할 필요 없이 효율적으로 처리 가능하다.

**Shape 확인:**  
실무에서 가장 유용한 검증 방법 중 하나는 **변수의 Shape와 해당 변수에 대한 그래디언트의 Shape가 일치하는지** 확인하는 것이다.

### 7. 구현 및 프레임워크
역전파는 모듈화된 방식으로 구현됩니다.
각 게이트는 `forward(inputs)`와 `backward(upstream_gradient)` API를 가진다.

**Caching:**  
`forward` 단계에서 사용한 입력값들을 저장해두어야 `backward` 단계에서 지역적 그래디언트를 계산할 때 사용할 수 있다.

**Caffe**와 같은 딥러닝 프레임워크의 소스 코드를 보면 각 레이어가 이 `forward`와 `backward` 패스를 명확하게 구현하고 있음을 알 수 있다.

### 8. Neural Networks의 정의
신경망은 단순한 선형 함수 $f = Wx$를 계층적으로 쌓아 올린 비선형 함수의 집합이다.

**2층 신경망 예시:**  
$f = W_2 \max(0 W_1x)$.


**Non-linearity:**  
레이어 사이에 **비선형 함수(예: Max(0 x) ReLU)** 를 넣는 것이 필수적이다. 비선형성이 없다면 여러 층을 쌓아도 결국 하나의 선형 함수로 축소되기 때문이다.

**템플릿의 결합:**  
선형 분류기가 클래스당 하나의 템플릿만 가졌던 한계를 극복한다. 첫 번째 가중치 층($W_1$)은 다양한 각도의 자동차나 다른 색상의 자동차와 같은 여러 템플릿을 학습하고 두 번째 층($W_2$)은 이 템플릿들의 점수를 가중 합산하여 최종 점수를 낸다. 이를 통해 하나의 클래스 내에서도 다양한 형태(Multi-modal)를 인식할 수 있게 됩니다.

### 9. Biological Inspiration
신경망은 인간의 뇌 구조에서 영감을 받았다.

**비유:**  
수상돌기(Dendrites)는 입력을 받고 세포체(Cell body)는 신호를 통합하며 축삭(Axon)은 신호를 전달한다. 이는 계산 노드에서 입력을 받아 가중치를 곱하고 합산한 뒤 활성화 함수를 거쳐 출력하는 과정과 유사하다.

**Activation Function:**  
시그모이드나 **ReLU** 같은 함수는 뉴런의 **발화율(Firing rate)** 을 모사한 것으로 볼 수 있다.

**주의사항:**  
실제 생물학적 뉴런은 인공 뉴런보다 훨씬 복잡하며 시냅스는 비선형 동역학 시스템이고 활성화 함수만으로 뉴런의 통신을 완벽히 설명할 수 없다. 따라서 뇌와의 비유는 매우 느슨한 수준으로만 이해해야 한다.
