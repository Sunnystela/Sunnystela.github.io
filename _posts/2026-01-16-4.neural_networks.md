---
layout: post
title: "[CS231n] 3. Loss Functions and Optimization"
date: 2026-01-15 09:50
categories: MyStudy CS231n
tags: cv CS231n
math: true
---

강의 주소: <br>
[CS231n Lecture 4. Introduction to Neural Networks](https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4)

자료: <br>
[깃허브 강의 자료](https://github.com/visionNoob/CS231N_17_KOR_SUB?tab=readme-ov-file)


### 1. 도입 및 행정 사항
강의는 Serena Young 박사 과정생이 진행하며, 몇 가지 행정적인 안내로 시작합니다. **과제 1(Assignment 1)**의 제출 기한이 4월 20일로 연장되었으며, 프로젝트 주제 선정에 도움을 주기 위해 TA들의 전문 분야가 Piaza에 게시되었습니다,. 또한, 모든 학생에게는 과제와 프로젝트를 위해 **Google Cloud 크레딧 100달러**가 제공됩니다.

### 2. 배경 지식 복습: 분류기와 손실 함수
이전 강의들에서는 가중치 $W$와 데이터 $X$를 입력으로 받아 클래스 점수를 출력하는 함수 $f(X, W)$를 정의하는 법을 배웠습니다.
*   **손실 함수(Loss Function):** SVM 손실 함수와 같이 모델의 예측에 대해 얼마나 만족하는지를 수치화합니다.
*   **규제화(Regularization):** 모델의 복잡도를 조절하여 일반화 성능을 높입니다.
*   **최적화(Optimization):** 손실을 최소화하는 $W$를 찾기 위해 경사 하강법(Gradient Descent)을 사용하여 손실 경사면의 가장 가파른 방향(음의 그래디언트)으로 이동합니다.
*   **그래디언트 계산:** 수치적 그래디언트는 구현이 쉽지만 느리고 근사치인 반면, **해석적 그래디언트(Analytic Gradient)**는 정확하고 빠르지만 유도가 어렵고 실수하기 쉽습니다,. 따라서 실제로는 해석적 그래디언트를 사용하되 수치적 그래디언트로 검증합니다.

### 3. 계산 그래프(Computational Graphs)
복잡한 함수의 해석적 그래디언트를 구하기 위해 **계산 그래프** 프레임워크를 사용합니다.
*   **정의:** 임의의 함수를 그래프로 표현하며, **노드(Node)**는 계산 단계를 나타냅니다.
*   **예시:** 선형 분류기의 경우, 가중치 곱셈, 힌지 손실 계산, 규제화 항 계산 등이 각각의 노드가 되며, 최종 손실 $L$은 이들의 합으로 표현됩니다.
*   **장점:** 함수가 아무리 복잡하더라도(CNN이나 Neural Turing Machine 등), 계산 그래프로 표현하면 **역전파(Backpropagation)** 기술을 사용하여 모든 변수에 대한 그래디언트를 재귀적으로 계산할 수 있습니다,,.

### 4. 역전파(Backpropagation)의 작동 원리
#### (1) 간단한 스칼라 예제 ($f(x, y, z) = (x + y)z$)
1.  **전방향 패스(Forward Pass):** 입력값($x=-2, y=5, z=-4$)을 사용하여 그래프를 따라 최종 출력값 $f=-12$를 구합니다. 중간 변수를 $q = x + y$라고 정의합니다.
2.  **역방향 패스(Backward Pass):** 출력에서 시작하여 뒤로 이동하며 **연쇄 법칙(Chain Rule)**을 적용합니다.
    *   최종 출력에 대한 그래디언트는 $\frac{df}{df} = 1$입니다.
    *   $\frac{df}{dz} = q = 3$.
    *   $\frac{df}{dq} = z = -4$.
    *   **연쇄 법칙 적용:** $y$에 대한 그래디언트 $\frac{df}{dy}$를 구할 때, 직접 연결되지 않았으므로 $\frac{df}{dq} \times \frac{dq}{dy}$를 계산합니다. $\frac{dq}{dy} = 1$이므로 $\frac{df}{dy} = -4 \times 1 = -4$가 됩니다,. $x$에 대해서도 동일하게 $\frac{df}{dx} = -4$를 얻습니다.

#### (2) 노드의 지역적 관점(Local Intuition)
각 노드는 오직 **자신의 직접적인 주변**만을 인식합니다,.
*   **지역적 그래디언트(Local Gradient):** 노드로 들어오는 입력에 대한 출력의 미분값으로, 전방향 패스 중에 미리 계산해 둘 수 있습니다,.
*   **업스트림 그래디언트(Upstream Gradient):** 뒤에서부터 전달되어 온 최종 손실에 대한 해당 노드 출력의 미분값입니다.
*   역전파 중에 노드는 **[업스트림 그래디언트] $\times$ [지역적 그래디언트]**를 수행하여 자신의 입력단으로 전달할 그래디언트를 계산합니다,.

### 5. 복잡한 예제: 시그모이드(Sigmoid) 함수
함수 $f(w, x) = \frac{1}{1 + e^{-(w_0x_0 + w_1x_1 + w_2)}}$를 계산 그래프로 풀어서 역전파를 수행합니다.
*   그래프는 곱셈, 덧셈, $x(-1)$, $\exp$, $+1$, $1/x$ 등의 아주 작은 단위 노드로 쪼개질 수 있습니다.
*   역전파 단계 예시:
    *   $1/x$ 노드: 지역적 그래디언트는 $-\frac{1}{x^2}$입니다.
    *   $+1$ 노드: 상수를 더하는 노드는 그래디언트를 그대로 통과시킵니다,.
    *   $\exp$ 노드: 지역적 그래디언트는 $e^x$입니다.
*   **노드의 세분화(Granularity):** 원한다면 여러 노드를 하나의 **시그모이드 게이트**로 묶을 수 있습니다,. 시그모이드 함수의 미분은 $\frac{d\sigma(x)}{dx} = (1 - \sigma(x))\sigma(x)$라는 깔끔한 형태를 가지므로, 이를 활용하면 계산을 단순화하고 그래프를 간결하게 만들 수 있습니다,.

### 6. 역전파의 패턴과 직관
*   **Add gate (덧셈):** 업스트림 그래디언트를 연결된 모든 입력으로 동일하게 복사해주는 **그래디언트 분배기(Distributor)** 역할을 합니다.
*   **Max gate:** 값이 더 큰 쪽으로만 그래디언트를 전달하고 다른 쪽은 0으로 만드는 **그래디언트 라우터(Router)** 역할을 합니다,.
*   **Mul gate (곱셈):** 업스트림 그래디언트에 상대방 입력의 값을 곱해주는 **그래디언트 스위처/스케일러(Switcher/Scaler)** 역할을 합니다.
*   **다중 연결 노드:** 한 변수가 여러 곳에 연결된 경우, 각 경로에서 돌아오는 **그래디언트들을 모두 합산**합니다 (다변수 연쇄 법칙),,.

### 7. 벡터화된 연산(Vectorized Operations)
입력이 스칼라가 아닌 벡터나 행렬일 경우에도 원리는 같습니다.
*   **야코비안 행렬(Jacobian Matrix):** 각 입력 요소에 대한 각 출력 요소의 편미분을 담은 행렬입니다.
*   입력이 4,096차원인 경우 야코비안은 $4,096 \times 4,096$ 크기가 되어 매우 크지만, 요소별 연산(예: ReLU)의 경우 야코비안은 **대각 행렬(Diagonal Matrix)** 형태가 되므로 실제로는 전체 행렬을 계산할 필요 없이 효율적으로 처리 가능합니다,,.
*   **Shape 확인:** 실무에서 가장 유용한 검증 방법 중 하나는 **변수의 Shape와 해당 변수에 대한 그래디언트의 Shape가 일치하는지** 확인하는 것입니다.

### 8. 구현 및 프레임워크
역전파는 모듈화된 방식으로 구현됩니다.
*   각 게이트는 `forward(inputs)`와 `backward(upstream_gradient)` API를 가집니다,.
*   **Caching:** `forward` 단계에서 사용한 입력값들을 저장해두어야 `backward` 단계에서 지역적 그래디언트를 계산할 때 사용할 수 있습니다.
*   **Caffe**와 같은 딥러닝 프레임워크의 소스 코드를 보면, 각 레이어(시그모이드, 컨볼루션 등)가 이 `forward`와 `backward` 패스를 명확하게 구현하고 있음을 알 수 있습니다,.

### 9. 신경망(Neural Networks)의 정의
신경망은 단순한 선형 함수 $f = Wx$를 계층적으로 쌓아 올린 비선형 함수의 집합입니다,.
*   **2층 신경망 예시:** $f = W_2 \max(0, W_1x)$.
*   **비선형성(Non-linearity):** 레이어 사이에 **비선형 함수(예: Max(0, x), ReLU)**를 넣는 것이 필수적입니다. 비선형성이 없다면 여러 층을 쌓아도 결국 하나의 선형 함수로 축소되기 때문입니다.
*   **템플릿의 결합:** 선형 분류기가 클래스당 하나의 템플릿만 가졌던 한계를 극복합니다,. 첫 번째 가중치 층($W_1$)은 다양한 각도의 자동차나 다른 색상의 자동차와 같은 여러 템플릿을 학습하고, 두 번째 층($W_2$)은 이 템플릿들의 점수를 가중 합산하여 최종 점수를 냅니다,,. 이를 통해 하나의 클래스 내에서도 다양한 형태(Multi-modal)를 인식할 수 있게 됩니다.

### 10. 생물학적 영감(Biological Inspiration)
신경망은 인간의 뇌 구조에서 느슨한 영감을 받았습니다.
*   **비유:** 수상돌기(Dendrites)는 입력을 받고, 세포체(Cell body)는 신호를 통합하며, 축삭(Axon)은 신호를 전달합니다,. 이는 계산 노드에서 입력을 받아 가중치를 곱하고 합산한 뒤 활성화 함수를 거쳐 출력하는 과정과 유사합니다,.
*   **활성화 함수(Activation Function):** 시그모이드나 **ReLU** 같은 함수는 뉴런의 **발화율(Firing rate)**을 모사한 것으로 볼 수 있습니다.
*   **주의사항:** 실제 생물학적 뉴런은 인공 뉴런보다 훨씬 복잡하며, 시냅스는 비선형 동역학 시스템이고 활성화 함수만으로 뉴런의 통신을 완벽히 설명할 수 없습니다,. 따라서 뇌와의 비유는 매우 느슨한 수준으로만 이해해야 합니다,.

강의는 신경망의 층을 깊게 쌓는 방식과 효율적인 벡터화 계산 방식을 요약하며 마무리됩니다,,. 다음 시간에는 **Convolutional Neural Networks(CNN)**에 대해 다룰 예정입니다.