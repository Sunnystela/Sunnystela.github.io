---
layout: post
title: GAN
date: 2026-01-26 12:50
categories: MyStudy 
tags: AI GAN GAN
math: true
---

>Jean Pouget-Abadie, Ian J. Goodfellow, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Yoshua Bengio, Aaron Courville (2014).Generative Adversarial Nets.
{: .prompt-tip }


## 1. Introduction
Generative Adversarial Nets (GANs)라는 새로운 프레임워크를 제안하며 이는 생성 모델(G)과 판별 모델(D)을 경쟁시키는 방식으로 작동한다. 마치 위조범과 경찰처럼 두 모델이 서로를 상대로 훈련하며 데이터 분포를 정확하게 복제하는 방법을 제시하는데 이는 기존의 복잡한 확률 계산 없이 역전파(backpropagation) 알고리즘만으로 훈련이 가능하다. 




생성 모델(G)이 데이터 분포를 정확하게 복제하도록 추정하는 새로운 프레임워크를 제안한다. 

두 모델을 동시에 훈련시킨다. 


생성 모델 (G): 데이터 분포를 포착한다. 

판별 모델 (D): 샘플이 실제 훈련 데이터에서 왔는지, 아니면 G에서 생성되었는지 확률을 추정한다. 


G는 D가 실수할 확률을 최대화하도록 훈련된다. 



복잡한 확률 계산 없이 backpropagation 알고리즘, 드롭아웃 알고리즘만으로  훈련이 가능하다. 

샘플 생성이나 훈련 시 Markov chains이나 근사 추론 네트워크가 필요 없다. 








기존 딥러닝의 성공은 주로 discriminative models에서 두드러졌으며 이는 역전파와 dropout 알고리즘에 기반한다. 



기존 딥 생성 모델의 어려움: 최대 우도 추정 등에서 발생하는 복잡한 확률 계산 때문에 영향력이 적었다. 


G는 위조지폐를 만드는 위조범, D는 이를 탐지하는 경찰에 비유되며 경쟁을 통해 위조품이 진짜와 구별 불가능해질 때까지 발전한다. 




## 2. Related work




RBMs/DBMs: 잠재 변수를 가진 비방향성 그래프 모델로 전역 합계(partition function) 계산이 어려워 Markov chain Monte Carlo에 의존한다. 



NCE (Noise-Contrastive Estimation): 판별 훈련 기준을 사용하지만, G 자체가 노이즈 분포 샘플과 G 생성 데이터를 구별하는 데 사용되어 학습이 느려지는 한계가 있다. 



GSN (Generative Stochastic Network): 파라미터화된 마르코프 체인을 정의하지만 GANs는 샘플링을 위해 마르코프 체인을 요구하지 않아 feedback loops 사용에 유리하다. 





## 3. Adversarial nets







G는 입력 노이즈 $z \sim p_z(z)$를 데이터 공간으로 매핑하는 미분 가능한 다층 퍼셉트론 $G(z; \theta_g)$이다. 



D는 단일 스칼라를 출력하는 다층 퍼셉트론 $D(x; \theta_d)$이다. 

D와 G는 $V(G, D)$를 minmax game을 한다. 

$$V(D,G) = E_{x \sim p_{data}(x)}[\log D(x)] + E_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$





G가 고정되었을 때 최적의 판별자 $D^*(x)$는 다음과 같다. 

$$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$

![alt](/assets/img/gan.algorithm.png)

Algorithm 1: D와 G를 번갈아 업데이트한다. 





D 업데이트:  
D를 최대화하기 위해 $k$ 스텝 동안 gradient ascent을 수행한다. 



G 업데이트:  
G를 최소화하기 위해 1 스텝 동안 gradient descent을 수행한다. 


G가 초기에 나쁠 경우 $\log(1 - D(G(z)))$ 항이 포화될 수 있다. 이 경우 G는 $\log D(G(z))$를 최대화하도록 훈련하여 더 강력한 기울기를 얻는다. 


![alt](/assets/img/gan.train.png)


## 4. Theoretical Results





글로벌 최적점: G와 D가 충분한 용량(capacity)을 가질 경우, 최소-최대 게임은 글로벌 최적점을 가지며, 이때 $p_g = p_{data}$가 된다. 



최적점에서의 값: 글로벌 최적점($p_g = p_{data}$)에서 $C(G)$ 값은 $-\log 4$이다. 



Jensen-Shannon Divergence (JSD): 최적화 기준 $C(G)$는 데이터 분포 $p_{data}$와 모델 분포 $p_g$ 사이의 JSD의 두 배와 관련이 있다. 

$$C(G) = -\log(4) + 2 \cdot JSD(p_{data} || p_g)$$





수렴 보장: JSD는 두 분포가 같을 때만 0이 되므로, $C(G)$의 글로벌 최솟값은 $p_g$가 $p_{data}$를 완벽하게 복제할 때 달성된다. 





## 5. 실험 결과 및 장단점 요약 






![alt](/assets/img/gan.table1.png)
Table 1. Parzen window-based log-likelihood estimates. 

실험 데이터셋:  
MNIST, TFD(Toronto Face Database), CIFAR-10을 사용했다. 

평가 방법:  
생성된 샘플에 대해 Parzen window를 피팅하여 log-likelihood를 추정했다. 


GANs는 DBN, StackedCAE, DeepGSN과 비교하여 경쟁적인 로그 우도 추정치를 보였다. 




![alt](/assets/img/gan.fig2.png)
생성된 샘플은 훈련 세트를 암기하지 않았으며 마르코프 체인 기반 샘플링과 달리 상관관계가 없다. 

## 6.  Advantages and disadvantages

GANs의 장점:  
마르코프 체인이 필요 없다.  
역전파만으로 훈련이 가능하다.  
piecewise linear units 활용에 유리하다.  
sharp distributions 표현이 가능하다. 



GANs의 단점:  
G와 D의 synchronization가 중요하다.  
D가 너무 많이 훈련되면 G의 학습이 어려워질 수 있다. 





## 7. Conclusions and future work






조건부 생성 모델:  
G와 D 모두에 조건 입력 $c$를 추가하여 $p(x|c)$를 얻을 수 있다. 



학습된 근사 추론(Learned approximate inference):  
G와 D 외에 추론 네트워크를 학습할 수 있다. 


준지도 학습(Semi-supervised learning):  
D의 features을 분류기 성능 향상에 활용할 수 있다. 



효율성 개선:  
훈련 가속화를 위한 더 나은 방법론을 찾는다. 

