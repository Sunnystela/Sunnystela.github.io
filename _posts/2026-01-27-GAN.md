---
layout: post
title: GAN
date: 2026-01-26 12:50
categories: MyStudy 
tags: AI GAN GAN
math: true
---

>Jean Pouget-Abadie, Ian J. Goodfellow, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Yoshua Bengio, Aaron Courville (2014).Generative Adversarial Nets.
{: .prompt-tip }


## 1. Introduction
Generative Adversarial Nets (GANs)라는 새로운 프레임워크를 제안하며 이는 생성 모델(G)과 판별 모델(D)을 경쟁시키는 방식으로 작동한다. 마치 위조범과 경찰처럼 두 모델이 서로를 상대로 훈련하며 데이터 분포를 정확하게 복제하는 방법을 제시하는데 이는 기존의 복잡한 확률 계산 없이 backpropagation 알고리즘만으로 훈련이 가능하다. 






생성 모델 (G): 데이터 분포를 학습한다.  
판별 모델 (D): 샘플이 실제 훈련 데이터에서 왔는지 아니면 G에서 생성되었는지 확률을 추정한다. 


G는 D가 실수할 확률을 최대화하도록 훈련된다. 



복잡한 확률 계산 없이 backpropagation 알고리즘, 드롭아웃 알고리즘만으로  훈련이 가능하다. 

샘플 생성이나 훈련 시 Markov chains이나 근사 추론 네트워크가 필요 없다. 

>기존 생성 모델들은 샘플 생성, 훈련에 복잡한 과정이 필요했는데 GAN은 단순한 순전파만으로 해결한다

>기존 생성 모델:  
샘플 생성: Markov chain으로 여러 단계 샘플링이 필요하다  
훈련: 근사 추론 네트워크로 확률 밀도 계산 → 복잡  
GAN:  
샘플 생성: G(z) 한 번 순전파 → 즉시 샘플 생성  
훈련: V(D,G) 미분 → backprop만으로 끝




기존 딥러닝의 성공은 주로 discriminative models에서 두드러졌다.  
최대 우도 추정 등에서 발생하는 복잡한 확률 계산 때문에 영향력이 적었다. 

>전통적 생성 모델이 데이터 전체 분포를 수학적 수식으로 직접 정의하려고 했기 때문이다. 이미지와 같은 복잡한 데이터에서는 이 계산을 컴퓨터가 처리하기에 너무 방대하고 복잡했다. 

G는 위조지폐를 만드는 위조범, D는 이를 탐지하는 경찰에 비유되며 경쟁을 통해 위조품이 진짜와 구별 불가능해질 때까지 발전한다. 









## 3. Adversarial nets







G는 입력 노이즈 $z \sim p_z(z)$를 데이터 공간으로 매핑하는 미분 가능한 다층 퍼셉트론 $G(z; \theta_g)$이다. 



D는 단일 스칼라를 출력하는 다층 퍼셉트론 $D(x; \theta_d)$이다. 

D와 G는 $V(G, D)$를 minmax game을 한다. 

$$V(D,G) = E_{x \sim p_{data}(x)}[\log D(x)] + E_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$





G가 고정되었을 때 최적의 판별자 $D^*(x)$는 다음과 같다. 

$$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$

![alt](/assets/img/gan.algorithm.png)

Algorithm 1: D와 G를 번갈아 업데이트한다. 





D 업데이트:  
D를 최대화하기 위해 $k$ 스텝 동안 gradient ascent을 수행한다. 



G 업데이트:  
G를 최소화하기 위해 1 스텝 동안 gradient descent을 수행한다. 


G가 초기에 나쁠 경우 $\log(1 - D(G(z)))$ 항이 포화될 수 있다. 이 경우 G는 $\log D(G(z))$를 최대화하도록 훈련하여 더 강력한 기울기를 얻는다. 


![alt](/assets/img/gan.train.png)


## 4. Theoretical Results

G와 D가 충분한 capacity을 가질 경우 minmax game은 글로벌 최적점을 가지며 이때 $p_g = p_{data}$가 된다. 


목적함수
글로벌 최적점($p_g = p_{data}$)에서 $C(G)$ 값은 $-\log 4$이다. 



Jensen-Shannon Divergence (JSD):  
최적화 기준 $C(G)$는 데이터 분포 $p_{data}$와 모델 분포 $p_g$ 사이의 JSD의 두 배와 관련이 있다. 

$$C(G) = -\log(4) + 2 \cdot JSD(p_{data} || p_g)$$




JSD는 두 분포가 같을 때만 0이 되므로 $C(G)$의 글로벌 최솟값은 $p_g$가 $p_{data}$를 완벽하게 복제할 때 달성된다. 





## 5. Experiments


![alt](/assets/img/gan.table1.png)
Table 1. Parzen window-based log-likelihood estimates. 
//확인

실험 데이터셋:  
MNIST, TFD(Toronto Face Database), CIFAR-10을 사용했다. 

평가 방법:  
생성된 샘플에 대해 Parzen window를 피팅하여 log-likelihood를 추정했다. 


>Parzen window  
데이터의 정확한 확률 분포 수식을 모를 때 가지고 있는 샘플을 바탕으로 분포의 모양을 추정하는 방법  
생성된 데이터 점 하나하나에 가우시안 커널을 올린다. 모두 더하면 데이터가 밀집된 곳은 높고 없는 곳은 낮다. 이게 모델이 학습한 확률 분포 근사치가 된다.
{: .prompt-info }

>왜 FID는 안했을까?
{: .prompt-warning }



GANs는 DBN, StackedCAE, DeepGSN과 비교하여 경쟁적인 로그 우도 추정치를 보였다. 

//확인


![alt](/assets/img/gan.fig2.png)
생성된 샘플은 훈련 세트를 암기하지 않았으며 마르코프 체인 기반 샘플링과 달리 상관관계가 없다. 
//확인

## 6.  Advantages and disadvantages

GANs의 장점:  
마르코프 체인이 필요 없다.  
역전파만으로 훈련이 가능하다.  
sharp distributions 표현이 가능하다. 

>VAE!를 불러오자  
기존 생성 모델은 데이터를 생성할 때 평균적인 값을 찾으려는 경향이 있어서 결과물이 다소 흐릿하게 나오는 경우를 봤다. 반면 GAN은 판별자가 아주 미세한 차이에도 가짜를 잡아내려 하기 때문에 생성자는 실제 데이터같은 선명한 경계와 디테일을 만든다.
{: .prompt-danger }


GANs의 단점:  
G와 D의 synchronization가 중요하다.  
D가 너무 많이 훈련되면 G의 학습이 어려워질 수 있다. 

>GAN은 생성자가 판별자가 경쟁하며 학습된다. D가 G를 처음부터 완벽하게 잡아내면 G는 어떤 부분이 부족해서 걸렸는지 파악할 수 없다. 

>원래 얻던 힌트: 기울기

>반대로 G가 너무 많이 훈련된다면? 그런 경우도 있는지
{: .prompt-warning }



## 7. Conclusions and future work

조건부 생성 모델:  
G와 D 모두에 조건 입력 $c$를 추가하여 $p(x|c)$를 얻을 수 있다. 



학습된 근사 추론(Learned approximate inference):  
G와 D 외에 추론 네트워크를 학습할 수 있다. 
>학습된 근사 추론:  
GAN에 인코더 기능을 추가하여 이미지에서 특징을 역으로 추출하겠다  
GAN은 노이즈z를 이미지 x로 변환하는 과정으로 작동한다. 근사 추론 네트워크는 이 반대 방향을 수행하는 추가로 학습시켜 모델이 데이터를 양방향으로 이해하게 만드는 것을 말한다

준지도 학습(Semi-supervised learning):  
D의 features을 분류기 성능 향상에 활용할 수 있다. 
>D를 확장된 분류기로 사용


효율성 개선:  
훈련 가속화를 위한 더 나은 방법론을 찾는다. 







<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

- [ ] 이해하고 

## 2. Related work




RBMs/DBMs: 잠재 변수를 가진 비방향성 그래프 모델로 전역 합계(partition function) 계산이 어려워 Markov chain Monte Carlo에 의존한다. 



NCE (Noise-Contrastive Estimation): 판별 훈련 기준을 사용하지만, G 자체가 노이즈 분포 샘플과 G 생성 데이터를 구별하는 데 사용되어 학습이 느려지는 한계가 있다. 



GSN (Generative Stochastic Network): 파라미터화된 마르코프 체인을 정의하지만 GANs는 샘플링을 위해 마르코프 체인을 요구하지 않아 feedback loops 사용에 유리하다. 
