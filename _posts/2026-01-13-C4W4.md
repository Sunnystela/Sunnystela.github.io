---
layout: post
title: "[DeepLearning.AI] Course4.W4 Deep Learning Specialization"
date: 2026-01-12 12:50
categories: MyStudy DeepLearning.AI
tags: DeepLearning.AI DeepLearning
math: true
---

[Convolutional Neural Networks (Course 4 of the Deep Learning Specialization)](https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)


## C4W4L01 What is Face Recognition


### 1.  **Liveness Detection**
**생체 감지** 는 대상이 살아있는 사람인지 아닌지(예: 사진)를 확인하는 기술이다.
    이는 Supervised Learning을 통해 구현되지만 본 강의에서는 생체 감지보다는 얼굴 인식 시스템을 만드는 방법에 더 집중할 예정이다.

### 2.  **핵심 용어 정의: 검증 vs 인식**
**Face Verification:**   
    '1:1 문제'이다. 입력 이미지와 ID가 주어졌을 때 해당 이미지가 그 사람이 맞는지 확인하는 작업이다.

**Face Recognition:**   
    검증보다 훨씬 어려운 문제로 데이터베이스에 있는 K명의 사람 중에서 입력된 사람이 누구인지 찾아내야 한다.

## C4W4L02 One Shot Learning

얼굴 인식 시스템 구축 시 직면하는 **One Shot Learning** 문제와 그 해결 방안인 **Similarity Function**에 대해 설명

### 1. One Shot Learning의 정의와 과제
**문제 상황:**   
대부분의 얼굴 인식 애플리케이션은 한 사람당 하나의 사진만 주어진 상태에서 그 사람을 정확히 인식해야 한다. 딥러닝 알고리즘은 역사적으로 단 하나의 훈련 예시만으로는 잘 작동하지 않는 경향이 있다.

>데이터베이스에 직원 4명의 사진이 하나씩만 저장되어 있다고할때 시스템은 카메라에 찍힌 사람이 저장된 4명 중 누구인지 식별하거나 만약 데이터베이스에 없는 외부인이라면 이를 감지해내야 한다.

### 2. 기존 CNN 접근의 한계
**Softmax 방식:**   
일반적인 방법은 이미지를 입력받아 Softmax 출력층을 통해 해당 사람이 누구인지를 분류하는 것이다.

**문제점:**  
    1.  **데이터 부족:**   
    훈련 세트가 너무 작아(사람당 사진 1장) 탄탄한 신경망을 훈련하기 어렵다.  
    2.  **확장성 부족:**   
    새로운 직원이 입사하여 인식해야 할 사람이 5명으로 늘어나면 출력층을 수정하고 네트워크를 처음부터 다시 훈련해야 하는 번거로움이 있다.

### 3. 해결책: Similarity Function 학습
**접근 방식:**   
단순히 사람을 분류하는 것이 아니라 두 이미지 간의 **차이(유사도)** 를 계산하는 함수 $d$를 신경망에 학습시킨다.  
    입력: 두 개의 이미지  
    출력: 두 이미지 사이의 차이를 나타내는 숫자

**판단 기준:**  
    두 이미지가 **같은 사람**일 경우: 작은 숫자를 반환.  
    두 이미지가 **다른 사람**일 경우: 큰 숫자를 반환.  
    **임계값($\tau$):**   
    반환된 차이 값이 특정 임계값($\tau$)보다 작으면 같은 사람으로 크면 다른 사람으로 예측한다.

### 4. 실제 Verification과 장점
**작동 원리:**   
새로운 사진이 들어오면 데이터베이스에 있는 기존 사진들과 일일이 함수 $d$를 통해 비교한다.
>데이터베이스의 사진들과 비교했을 때 다른 사람들과는 큰 값(예: 10)이 나오지만 특정 인물과 비교했을 때 작은 값이 나온다면 그 사람으로 식별한다.  
반대로 데이터베이스의 모든 사진과 비교했을 때 모두 큰 값이 나온다면 등록되지 않은 사람으로 판단한다.

**확장성(Scalability):**   
새로운 팀원이 들어올 경우 신경망을 재훈련할 필요 없이 데이터베이스에 그 사람의 사진을 추가하기만 하면 된다. 함수 $d$는 이미 두 이미지의 유사성을 판단하는 법을 알고 있기 때문에 새로운 사람에게도 그대로 적용 가능하다.


**ai:**  

기존의 Softmax 방식이 학급의 모든 학생 얼굴을 통째로 **암기**해서 출석을 부르는 방식이라면(전학생이 오면 처음부터 다시 외워야 함) 원샷 학습 방식은 경비원이 **신분증 대조**를 하는 것과 같습니다. 경비원은 모든 사람의 얼굴을 외울 필요 없이 신분증 사진과 실물 얼굴이 일치하는지(유사도)만 확인하면 되므로 처음 보는 사람이 와도 쉽게 판별할 수 있습니다.

## C4W4L03 Siamese Network

### **1. 샴 네트워크(Siamese Network)의 기본 구조와 인코딩**
강의는 두 이미지가 얼마나 같거나 다른지 판단하는 함수 $d$를 만들기 위해 **샴 네트워크**를 도입하는 것으로 시작한다.
일반적인 합성곱 신경망(CNN)과 유사하게 합성곱 풀링 완전 연결 층을 거치지만 마지막에 분류를 위한 소프트맥스(Softmax) 층을 사용하지 않습니다.
대신 신경망의 최종 결과물로 128개의 숫자 등으로 구성된 **특성 벡터(feature vector)**를 출력한다.
이 벡터를 입력 이미지 $x^{(1)}$의 **인코딩(encoding)** 즉 $f(x^{(1)})$이라고 부릅니다.

**2. 이미지 비교 방식과 거리 계산**
두 장의 사진(예: $x^{(1)}$과 $x^{(2)}$)을 비교하기 위해 이 네트워크를 어떻게 활용하는지 설명한다.
두 사진을 비교할 때는 **동일한 파라미터를 가진 동일한 신경망**에 각각의 이미지를 통과시킵니다.
그 결과 두 번째 이미지에 대한 인코딩 벡터 $f(x^{(2)})$를 얻게 됩니다.
두 이미지 사이의 거리 $d$는 **두 인코딩 벡터 차이의 노름(norm)**으로 정의한다.
이처럼 두 개의 입력에 대해 독립적으로 동일한 신경망을 실행하여 비교하는 구조를 **샴 신경망(Siamese Neural Network)**이라고 부르며 이는 DeepFace 시스템(Taigman et al.) 등의 연구에 기반을 두고 있습니다.

**3. 신경망의 학습 목표**
샴 네트워크를 훈련시킨다는 것은 좋은 인코딩 함수 $f$를 만드는 파라미터를 학습하는 것이다. 학습의 핵심 목표는 다음과 같습니다.
**같은 사람인 경우:**   
두 이미지 $x^{(i)}$와 $x^{(j)}$가 같은 사람이라면 인코딩 사이의 거리가 **작아야 한다**.
**다른 사람인 경우:**   
두 이미지가 다른 사람이라면 인코딩 사이의 거리가 **커야 한다**.
이 조건을 만족시키기 위해 역전파(backpropagation)를 사용하여 신경망의 파라미터를 업데이트한다.

**4. 다음 단계 예고**
영상은 이러한 직관적인 목표를 달성하기 위해 실제로 어떤 **목적 함수(objective function)**를 사용하여 학습해야 하는지에 대한 질문을 던지며 마무리됩니다. 이에 대한 구체적인 방법론인 '삼중항 손실(Triplet Loss)'은 다음 강의에서 다루어집니다.

**ai:**  

샴 네트워크는 마치 **'몽타주 전문가'**와 같습니다. 사진 자체를 픽셀 단위로 비교하는 것이 아니라 사진을 보고 "눈 사이 거리" "턱의 각도" 등 핵심 특징을 128개의 수치(인코딩)로 요약해 냅니다. 만약 두 사진이 같은 사람이라면 전문가가 그려낸 두 몽타주의 수치도 거의 비슷할 것이고 다른 사람이라면 수치 차이가 크게 날 것이다.

## C4W4L04 Triplet loss


이 강의는 얼굴 인식을 위한 신경망의 인코딩을 훈련시키는 **삼중항 손실(Triplet Loss)** 함수의 정의와 작동 원리 그리고 효과적인 학습 방법을 설명하고 있습니다.

### 1. 삼중항(Triplet)의 개념과 목표
얼굴 이미지에 대한 좋은 인코딩(encoding)을 얻기 위해 신경망은 한 번에 세 개의 이미지를 비교한다. 이를 **삼중항(Triplet)**이라고 하며 다음과 같이 구성됩니다.
**앵커(Anchor A):**   
기준이 되는 이미지
**긍정(Positive P):**   
앵커와 같은 사람의 이미지
**부정(Negative N):**   
앵커와 다른 사람의 이미지

학습의 목표는 앵커와 긍정 이미지 사이의 거리 $d(A P)$는 가깝게 만들고 앵커와 부정 이미지 사이의 거리 $d(A N)$은 멀게 만드는 것이다.

### 2. 마진(Margin $\alpha$)의 도입
단순히 $d(A P) \le d(A N)$이라는 조건만 설정할 경우 신경망이 모든 인코딩을 0으로 만들거나 모든 이미지의 인코딩을 똑같이 출력하여 수식을 만족시키는 '자명한 해(trivial solution)'를 도출할 위험이 있습니다.

이를 방지하기 위해 **마진(margin $\alpha$)**이라는 하이퍼파라미터를 도입한다.
목표 수식: $||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha \le 0$
이 마진은 긍정 이미지 쌍과 부정 이미지 쌍 사이의 간격을 최소한 $\alpha$만큼 벌려놓도록 강제하여 신경망이 유의미한 학습을 하도록 돕습니다.

### 3. 삼중항 손실 함수(Loss Function)의 정의
단일 삼중항에 대한 손실 함수는 다음과 같이 정의됩니다.

$L(A P N) = \max(0 ||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha)$

이 수식의 의미는 다음과 같습니다.
만약 조건($\le 0$)을 만족하면 손실은 **0**이 됩니다.
조건을 만족하지 못하면(양수이면) 그 값이 그대로 손실이 되어 신경망이 이를 줄이는 방향(경사 하강법)으로 학습하게 됩니다.
전체 비용 함수(Cost Function)는 훈련 세트에 있는 모든 삼중항에 대한 손실의 합이다.

### 4. 훈련 데이터 구성: '어려운 삼중항(Hard Triplets)' 선택
훈련을 위해서는 한 사람당 여러 장의 이미지가 있는 데이터 세트가 필요한다(앵커와 긍정 이미지를 쌍으로 묶어야 하기 때문이다).

이때 삼중항을 **무작위(Random)**로 선택하면 문제가 발생한다.
임의의 두 사람은 이미 서로 매우 다르게 생겼을 확률이 높습니다. 즉 $d(A N)$이 $d(A P)$보다 훨씬 커서 손실 값이 0이 되는 경우가 많습니다. 이렇게 되면 경사 하강법이 할 일이 없어 학습이 제대로 이루어지지 않습니다.

따라서 **'학습하기 어려운(Hard)' 삼중항**을 골라야 한다.
$d(A P)$와 $d(A N)$의 값이 서로 비슷해서 신경망이 헷갈리기 쉬운 조합을 선택한다.
이러한 데이터를 사용해야 학습 알고리즘이 앵커와 부정 이미지를 멀어지게 하려고 노력하게 되며 학습 효율이 높아집니다.
이 개념은 FaceNet 논문(Florian Schroff et al.)에서 소개된 방식이다.

### 5. 대규모 데이터와 사전 학습 모델 활용
상업적인 얼굴 인식 시스템은 수백만에서 1억 개 이상의 이미지를 사용하여 대규모로 훈련됩니다. 개인이 이러한 데이터를 구하거나 처음부터 훈련시키는 것은 어렵습니다.

따라서 강의에서는 처음부터 직접 훈련하기보다는 다른 기관이 대규모 데이터로 미리 훈련시켜 공개한 **사전 학습 모델(pre-trained model)**을 다운로드하여 사용하는 것을 권장한다.

**ai:**  

'어려운 삼중항(Hard Triplets)'을 고르는 과정은 운동선수가 훈련 상대를 고르는 것과 비슷한다. 실력 차이가 너무 많이 나는 초보자(쉬운 삼중항)와 경기하면 이미 이기고 있기 때문에 실력이 늘지 않습니다. 자신과 실력이 비슷한 라이벌(어려운 삼중항)과 훈련해야 자신의 약점을 파악하고 기량을 더 발전시킬 수 있는 것과 같은 원리이다.


## C4W4L05 Face Verification

이 강의는 얼굴 인식 문제를 해결하기 위해 이전에 배운 **삼중항 손실(Triplet Loss)** 대신 **이진 분류(Binary Classification)** 문제로 접근하는 방법을 설명하고 있습니다.

### 1. 얼굴 인식을 이진 분류 문제로 변환 (영상 초반)
삼중항 손실 함수 외에 신경망을 훈련하는 또 다른 방법은 얼굴 인식 문제를 **이진 분류(Binary Classification)** 문제로 취급하는 것이다.

**샴 신경망(Siamese Network) 활용:**   
두 개의 샴 신경망 쌍을 이용해 각각의 이미지에 대한 임베딩(예: 128차원 벡터)을 계산한다.
**로지스틱 회귀 입력:**   
계산된 두 임베딩을 로지스틱 회귀 유닛(Logistic Regression Unit)에 입력한다.
**예측 목표:**   
두 이미지가 **같은 사람일 경우 1** **다른 사람일 경우 0**을 출력하도록 학습한다.
**입력값 계산:**   
단순히 임베딩을 입력하는 것이 아니라 두 이미지의 인코딩 벡터 간 **차이의 절댓값**을 계산하여 입력값(특성)으로 사용한다.

### 2. 구체적인 수식과 변형 (영상 중반)
이진 분류를 위한 구체적인 수식 적용 방법과 대안적인 계산법을 설명한다.

**학습 파라미터:**   
128개의 인코딩 차이 값들을 특성(Feature)으로 간주하고 여기에 일반적인 로지스틱 회귀처럼 가중치($w_i$)와 편향($b$)을 적용하여 훈련한다.
**변형된 유사도 공식 (카이제곱):**   
인코딩 차이의 절댓값 대신 다른 공식을 사용할 수도 있습니다. 예를 들어 **카이제곱($\chi^2$) 유사도** 공식($\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k}$)이 있으며 이는 DeepFace 논문에서 연구된 바 있습니다.

### 3. 계산 효율성을 위한 팁: 미리 계산하기 (Pre-computation) (영상 후반)
실제 시스템 배포 시 계산 속도를 높이기 위한 중요한 최적화 방법을 소개한다.

**입력 데이터 구조:**   
훈련 시 입력값은 두 개의 이미지 쌍($x$)이며 출력값($y$)은 유사도에 따른 0 또는 1이다.
**임베딩 미리 계산:**   
데이터베이스에 저장된 기존 이미지(예: 직원 사진)들의 임베딩은 매번 다시 계산할 필요가 없습니다.
    **작동 방식:**   
    새로운 사람(새 이미지)이 들어왔을 때 위쪽 신경망을 통해 그 사람의 인코딩만 계산한다. 이를 데이터베이스에 **이미 계산되어 저장된 인코딩**과 비교하여 예측값($y$)을 구한다.
    **장점:**   
    원시 이미지를 매번 처리하지 않아도 되므로 데이터베이스가 클 경우 계산량을 상당히 줄일 수 있습니다.

### 4. 결론 및 요약
이진 분류 접근법을 통한 얼굴 검증 시스템 구축을 정리한다.

**범용성:**   
임베딩을 미리 계산해두는 방식은 삼중항 손실 함수를 사용할 때나 이진 분류 방식을 사용할 때 모두 적용 가능한다.
**지도 학습으로의 전환:**   
얼굴 검증을 지도 학습(Supervised Learning)으로 다루기 위해 세 쌍(Triplet) 대신 **쌍(Pair)**으로 된 훈련 세트를 만듭니다 (같은 사람=1 다른 사람=0).
**성능:**   
이 방식은 역전파를 통해 샴 신경망을 훈련시키며 얼굴 검증 및 **원샷 학습(One-shot learning)** 시스템을 구축하는 데 꽤 잘 작동한다.


**ai:**  

이 방식은 두 사람의 지문이 같은지 확인할 때 지문 사진 두 장을 겹쳐보는 것(픽셀 비교)이 아니라 지문의 **핵심 특징(소용돌이 수 갈라짐 등)**을 수치로 변환(임베딩)한 뒤 그 **수치들의 차이**가 얼마나 작은지를 계산해 동일인 여부를 판단(이진 분류)하는 것과 같습니다. 이때 기존 등록된 사람들의 특징 수치는 미리 계산해 장부에 적어두면(Pre-computation) 매번 사진을 다시 분석할 필요가 없어 확인 속도가 훨씬 빨라집니다.


## C4W4L06 Neural Style Transfer

1.  **개요 및 소개**: 신경망 스타일 변형은 최근 신경망 응용 분야 중 가장 흥미로운 기술 중 하나이며 학습자는 이를 프로그래밍 과제를 통해 직접 구현해 보게 됩니다.
2.  **작동 예시**:
    **스탠포드 대학교 사진**을 내용(content)으로 하고 반 고흐의 **'별이 빛나는 밤에'**를 스타일(style)로 적용하면 반 고흐 화풍으로 그려진 학교 전경 이미지가 생성됩니다.
    **샌프란시스코 금문교 사진**을 내용으로 하고 **파블로 피카소의 그림**을 스타일로 적용하면 피카소 스타일의 금문교 이미지가 생성됩니다.
3.  **핵심 용어 정의**: 구현 과정을 설명하기 위해 각 이미지를 다음과 같은 기호로 칭한다.
    **C** (Content): 변형할 내용 이미지
    **S** (Style): 적용할 스타일 이미지
    **G** (Generated): 생성될 결과 이미지
4.  **구현 원리 맛보기**: 신경망 스타일 변형을 구현하기 위해서는 신경망의 **얕은 층(shallow layers)**과 **깊은 층(deep layers)**에서 추출되는 특성들을 모두 살펴봐야 한다.
5.  **향후 학습 안내**: 구체적인 구현 방법을 배우기에 앞서 다음 영상에서는 신경망의 각 층이 실제로 무엇을 계산하는지에 대해 직관적으로 이해하는 시간을 가질 예정이다.



## C4W4L07 What are deep CNs learning?

**1. 시각화 방법론 (Introduction)**
신경망의 특정 층(Layer)에 있는 은닉 유닛(Hidden Unit)이 무엇을 보고 있는지 알아내기 위해 훈련 데이터셋의 이미지들을 신경망에 통과시킵니다.
그 후 해당 유닛의 활성값(Activation)을 최대화하는 9개의 이미지 조각(Patches)을 찾아냅니다.
이 방식은 Matthew Zeiler와 Rob Fergus의 논문 "Visualizing and understanding convolutional networks"에 기반을 두고 있습니다.

**2. 1층(Layer 1)의 학습 내용**
1층의 은닉 유닛들은 전체 이미지 중 비교적 작은 영역(수용영역)만 봅니다.
시각화 결과 1층은 주로 **모서리(Edge)**나 **선(Line)**의 각도 또는 특정 **색깔** 구성과 같은 단순한 특징들을 찾습니다.

**3. 2층(Layer 2)의 학습 내용**
깊은 층으로 갈수록 은닉 유닛은 이미지의 더 큰 부분을 보게 됩니다.
2층은 1층보다 조금 더 복잡한 모양과 패턴을 감지한다.
예를 들어 세로선이 많은 텍스처 원형 모양 아주 얇은 세로선 등을 찾는 유닛들이 발견됩니다.

**4. 3층(Layer 3)의 학습 내용**
3층부터는 더 복잡한 물체의 패턴이 드러납니다.
이미지의 특정 부분에서 둥근 물체(자동차 바퀴 등) 사람의 형체 혹은 벌집 모양과 같은 특정 텍스처를 검출하는 유닛들이 확인됩니다.

**5. 4층(Layer 4)의 학습 내용**
4층은 상당히 정교한 특징이나 패턴을 감지한다.
특정 종류의 개(Dog)를 인식하는 탐지기 물(Water) 새의 다리 등을 검출하는 것으로 보이는 유닛들이 있습니다.

**6. 5층(Layer 5)의 학습 내용**
가장 깊은 5층에서는 더욱 다양한 종류의 사물과 정교한 객체를 검출한다.
다양한 생김새의 개들 키보드나 점무늬 배경 통(Barrel) 꽃 등을 인식하는 유닛들이 존재한다.

**7. 결론 및 요약**
신경망은 얕은 층(1층)에서는 모서리와 같은 단순한 특징을 감지하는 것으로 시작한다.
층이 깊어질수록 텍스처를 거쳐 복잡한 물체(Object)를 검출하는 방향으로 학습이 진행됩니다.
이러한 직관은 추후 '신경망 스타일 변형(Neural Style Transfer)' 알고리즘을 이해하고 구현하는 데 도움을 줍니다.

## C4W4L08 Cost Function

**1. 문제 정의 및 목표 설정**
신경망 스타일 변형(Neural Style Transfer) 시스템을 구현하기 위해 **생성 이미지(Generated Image G)가 얼마나 좋은지를 측정하는 비용 함수(Cost Function $J$)를 정의**해야 한다. 이 문제의 핵심은 내용 이미지(C)와 스타일 이미지(S)가 주어졌을 때 이를 바탕으로 새로운 이미지 G를 만들어내는 것이다.

**2. 비용 함수($J$)의 구성 요소**
비용 함수 $J(G)$는 크게 두 가지 부분으로 구성되며 이를 최소화하는 방향으로 학습이 진행됩니다.
**내용 비용(Content Cost)**: 생성 이미지(G)와 내용 이미지(C)의 **내용이 얼마나 비슷한지**를 측정한다.
**스타일 비용(Style Cost)**: 생성 이미지(G)와 스타일 이미지(S)의 **스타일이 얼마나 비슷한지**를 측정한다.
여기에 하이퍼파라미터 $\alpha$(알파)와 $\beta$(베타)를 사용하여 두 비용 사이의 상대적 가중치를 조절한다.

**3. 알고리즘 초기화**
이 알고리즘은 Leon Gatys Alexander Ecker Matthias Bethge의 논문에 기반한다. 이미지를 생성하기 위해 가장 먼저 **생성 이미지 G를 무작위로 초기화**한다. 예를 들어 100x100x3 차원의 **화이트 노이즈(White Noise)** 같은 이미지로 시작하게 됩니다.

**4. 학습 및 최적화 과정 (경사 하강법)**
무작위로 초기화된 이미지에 대해 비용 함수 $J(G)$를 정의한 후 **경사 하강법(Gradient Descent)**을 사용하여 비용 함수를 최소화한다.
이 과정은 실제로 이미지 G의 **픽셀값들을 업데이트**하는 것이다.
학습이 진행됨에 따라 초기의 노이즈 이미지는 점차 내용 이미지의 형태를 갖추면서 스타일 이미지의 화풍이 입혀진 결과물로 변하게 됩니다.

강의의 마지막 부분에서는 이번 영상이 전반적인 개요였음을 언급하며 다음 영상들에서 내용 비용 함수와 스타일 비용 함수를 구체적으로 정의하는 방법을 다룰 것임을 예고한다.


**ai:**  

이 과정은 **요리 대회**에 참가하는 것과 비슷한다.
**내용 이미지(C)**는 '김치찌개'라는 정해진 **메뉴(내용)**이다.
**스타일 이미지(S)**는 '미슐랭 3스타 셰프'의 **플레이팅 방식(스타일)**이다.
**초기화**는 아무 재료나 섞어놓은 엉망인 상태에서 시작하는 것이다.
**비용 함수($J$)**는 심사위원이 "맛이 김치찌개랑 얼마나 비슷한가?"와 "플레이팅이 셰프랑 얼마나 비슷한가?"를 점수 매기는 기준이다.
**경사 하강법**은 심사위원의 평가를 듣고 요리를 조금씩 수정해 나가며 완벽한 '미슐랭 스타일의 김치찌개'를 완성해가는 과정이다.


## C4W4L09 Content Cost Function

**영상 요점 정리 (시간 순서)**

1.  **비용 함수의 구성**
    신경망 스타일 변형(Neural Style Transfer) 알고리즘의 전체 비용 함수는 **'내용 비용(Content Cost)'**과 **'스타일 비용(Style Cost)'** 두 가지 요소로 이루어져 있습니다. 이번 영상에서는 그중 내용 비용 함수를 정의하는 방법에 집중한다.

2.  **은닉층 $l$의 선택**
    내용 비용을 계산하기 위해 신경망의 특정 은닉층 $l$을 선택해야 한다.
    **얕은 층(작은 $l$):**   
    선택 시 생성된 이미지가 원본의 픽셀 값과 매우 유사하게 만들어집니다.
    **깊은 층(큰 $l$):**   
    선택 시 이미지의 구체적인 내용(예: 개가 있는지 여부)에 집중한다.
    **결론:**   
    실제 구현에서는 너무 얕지도 깊지도 않은 **중간 단계의 층**을 선택하는 것이 일반적이다.

3.  **활성값을 이용한 유사도 측정**
    사전 훈련된 합성곱 신경망(예: VGG 네트워크)을 사용하여 내용 이미지(C)와 생성된 이미지(G)의 유사도를 측정한다.
    선택한 $l$층에서의 활성값(activation)을 각각 $a^{[l]C}$ $a^{[l]G}$라고 할 때 이 두 값이 비슷하다면 두 이미지의 내용이 유사하다고 볼 수 있습니다.

4.  **내용 비용 함수($J_{content}$)의 정의**
    내용 비용 함수는 두 이미지의 활성값 차이를 제곱하여 합한 값(L2 노름의 제곱)으로 정의한다.
    수식 앞에 붙는 정규화 상수(예: 1/2)는 전체 비용 함수에서 하이퍼파라미터 $\alpha$를 통해 비율을 조정할 수 있으므로 구체적인 숫자는 크게 중요하지 않습니다.

5.  **최적화의 효과**
    경사 하강법을 통해 이 비용 함수(J)를 최소화하면 알고리즘은 은닉층의 활성값이 내용 이미지와 유사해지도록 이미지(G)를 변형시킵니다.
    다음 영상에서는 스타일 비용 함수에 대해 다룰 예정이다.

**ai**

**내용 비용 함수**를 설정하는 것은 화가에게 그림을 의뢰할 때 **"밑그림(구조)은 이것과 똑같이 그려주세요"**라고 요청하는 것과 같습니다.
**얕은 층**을 기준으로 삼는 건 "모눈종이 대고 똑같이 베껴 그려라(픽셀 일치)"라고 하는 것이고
**깊은 층**을 기준으로 삼는 건 "강아지가 있다는 사실만 같으면 된다"라고 하는 것이다.
따라서 강의에서는 이 둘의 균형을 맞추기 위해 **중간 층**을 기준으로 삼아 전체적인 형태와 구조가 원본과 비슷하게 유지되도록 유도한다.


## C4W4L10 Style Cost Function


**영상 요점 정리 (시간 순서)**

1.  **스타일(Style)의 정의**
    이전 강의에서 다룬 '내용 비용 함수'에 이어 이번에는 '스타일 비용 함수'를 정의한다.
    이미지의 스타일을 정의하기 위해 신경망의 특정 층($l$)을 선택하고 해당 층에 있는 **서로 다른 채널들의 활성값(activation) 간의 상관 관계(correlation)**를 측정한다.

2.  **상관 관계의 직관적 이해**
    예를 들어 빨간색 채널이 '세로선 텍스처'를 감지하고 노란색 채널이 '주황색 색조'를 감지한다고 가정해 봅니다.
    두 채널의 상관 관계가 높다는 것은 이미지의 특정 부분에 세로선 텍스처가 나타날 때 주황색 색조도 함께 나타난다는 것을 의미한다.
    즉 서로 다른 특성(텍스처 색감 등)들이 이미지 내에서 얼마나 자주 함께 등장하는지를 측정함으로써 그 이미지의 '스타일'을 수치화할 수 있습니다.

3.  **스타일 행렬 (그램 행렬 Gram Matrix)**
    스타일을 수학적으로 계산하기 위해 **스타일 행렬(Style Matrix)**을 정의한다. 선형대수학에서는 이를 **그램 행렬(Gram Matrix)**이라고 부르며 대문자 $G$로 표기한다.
    이 행렬은 $n_c \times n_c$ (채널 수 $\times$ 채널 수) 크기의 정사각행렬이다.
    행렬의 요소 $G_{kk'}$는 채널 $k$와 채널 $k'$의 활성값들을 위치($i j$)별로 곱한 뒤 모두 더하여 계산한다. 이는 수학적으로 비정규화된 교차 공분산(unnormalized cross-covariance)에 해당한다.

4.  **스타일 비용 함수 계산**
    스타일 이미지(S)와 생성된 이미지(G)에 대해 각각 스타일 행렬(그램 행렬)을 계산한다.
    특정 층 $l$에서의 스타일 비용 함수는 두 이미지의 **스타일 행렬 간의 차이를 제곱(프로베니우스 노름의 제곱)**한 값으로 정의한다.
    여기에 정규화 상수가 포함되지만 이는 나중에 곱해질 가중치 파라미터($\beta$) 때문에 크게 중요하지 않습니다.

5.  **전체 비용 함수와 최적화**
    더 나은 시각적 결과를 얻기 위해 하나의 층만 사용하는 것이 아니라 여러 층(단순한 특성을 보는 초기 층부터 복잡한 특성을 보는 깊은 층까지)의 스타일 비용을 가중치($\lambda$)를 두어 합산한다.
    최종적으로 전체 비용 함수 $J(G)$는 **(알파 $\times$ 내용 비용) + (베타 $\times$ 스타일 비용)**으로 구성됩니다.
    경사 하강법(Gradient Descent)을 사용하여 이 전체 비용 함수를 최소화함으로써 원하는 스타일과 내용을 가진 합성 이미지를 생성할 수 있습니다.

---

**이해를 돕기 위한 비유**

**스타일(Style)**을 정의하는 과정을 **'요리 레시피 분석'**에 비유할 수 있습니다.

**내용(Content)**이 '소고기' '양파' 같은 재료 자체라면
**스타일(Style)**은 '재료들이 함께 사용되는 패턴'이다.
    어떤 요리사가 '매운 고춧가루'(채널 A)를 쓸 때마다 항상 '참기름'(채널 B)을 같이 쓴다면 두 재료 간의 **상관 관계**가 높은 것이며 이것이 그 요리사만의 **맛의 스타일**이 됩니다.
**그램 행렬(Gram Matrix)**은 이러한 재료 조합의 패턴 목록을 표로 만든 것과 같습니다.
우리는 생성된 요리(이미지 G)의 맛의 패턴 표가 유명 셰프의 요리(이미지 S)의 패턴 표와 최대한 비슷해지도록 조절하는 것이다.


## C4W4L11 1D and 3D Generalizations
지금까지 배운 2D 이미지 처리용 CNN 아이디어를 다른 차원의 데이터에 어떻게 적용할 수 있는지.

### 1. 강의 도입 및 2D 합성곱 복습
지금까지의 강의는 이미지 인식 얼굴 인식 스타일 변형 등 주로 **2D 데이터(이미지)** 를 다루는 합성곱 신경망에 초점을 맞췄습니다.
하지만 CNN의 핵심 아이디어는 2D뿐만 아니라 **1D 및 3D 데이터**에도 동일하게 적용될 수 있다.

**2D 복습:**   
$14 \times 14$ 이미지를 $5 \times 5$ 필터와 합성곱하면 $10 \times 10$ 결과가 나오며 채널이 늘어나도 depth 방향으로 연산이 확장되는 원리.

### 2. 1D 데이터로의 일반화 
**데이터 예시:**   
심전도(EKG) 시그널은 시간에 따른 전압 변화를 보여주는 대표적인 **1D 데이터**이다.

**작동 원리:**  
    입력 데이터는 $14 \times 14$ 행렬이 아니라 **14차원의 숫자 리스트(배열)** 형태를 가진다.
    여기에 **1차원 필터(예: 5차원)**를 사용하여 시계열 데이터의 여러 위치를 훑으며 합성곱을 수행한다.
    $14$차원 입력에 $5$차원 필터를 적용하면 $10$차원 결과가 나오며 16개의 필터를 사용하면 $10 \times 16$ 차원의 출력을 얻는다.

**활용:**   
이 방식은 심전도에서 특정 심장 박동 패턴을 검출하거나 다양한 시계열 데이터를 분석하는 데 사용될 수 있습니다.

**참고:**   
시퀀스 데이터 처리를 위해 다음 강의에서 다룰 RNN LSTM이 주로 쓰이지만 1D 합성곱 신경망도 유용한 대안이 될 수 있다.

### 3. 3D 데이터로의 일반화 (예: CT 스캔 영상)
**데이터 예시:**   
CT 스캔은 신체의 여러 단면을 찍어 합친 것으로 높이 너비 깊이를 가진 **3D 부피(Volume)** 데이터이다.

**작동 원리:**  
    입력 데이터는 3차원 블록 형태(예: $14 \times 14 \times 14$)이다.
    여기에 **3차원 필터(예: $5 \times 5 \times 5$)** 를 적용하여 부피 전체를 훑으며 특징을 검출한다.
    $14 \times 14 \times 14$ 입력에 $5 \times 5 \times 5$ 필터를 적용하면 $10 \times 10 \times 10$ 부피의 결과가 나오며 필터 개수(예: 16개)에 따라 채널이 추가된다($10 \times 10 \times 10 \times 16$).

**다른 예시:**   
**영화 데이터** 또한 2D 이미지가 시간 축으로 쌓인 3D 데이터로 볼 수 있으며 이를 통해 영상 속 움직임을 감지하는 데 활용할 수 있다.

이미지 데이터가 가장 보편적이라 2D CNN이 주를 이루지만 1D 및 3D 모델 또한 특정 분야에서 매우 유용한다.



