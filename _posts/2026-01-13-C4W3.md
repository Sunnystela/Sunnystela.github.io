---
layout: post
title: "[DeepLearning.AI] Course4.W3 Deep Learning Specialization"
date: 2026-01-12 12:50
categories: MyStudy DeepLearning.AI
tags: DeepLearning.AI DeepLearning
math: true
---

[Convolutional Neural Networks (Course 4 of the Deep Learning Specialization)](https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)


## C4W3L01 Object Localization

### **1. 물체 Localization의 정의**
**이미지 Classification:**  
이미지를 보고 "이것은 자동차다"라고 판별하는 작업.

**Classification with Localization:**  
물체의 종류를 판별(레이블링)하는 것뿐만 아니라 물체 주변에 **Bounding Box**를 그려 위치까지 정확히 파악하는 작업이다.

**물체 Detection:**  
로컬리제이션이 보통 이미지 중앙에 있는 '단일 물체'를 다루는 반면 감지 문제는 하나의 이미지 안에 있는 서로 다른 종류의 '여러 물체'를 모두 찾아내는 더 복잡한 문제이다. 먼저 단일 물체의 로컬리제이션을 다룬다.

### **2. 신경망 구조의 변형과 경계 상자(Bounding Box)**
기존 이미지 분류 문제는 CNN을 거쳐 Softmax 층에서 클래스를 예측한다.
로컬리제이션을 위해서는 이 신경망이 클래스 확률 외에 **경계 상자를 정의하는 4개의 숫자($b_x b_y b_h b_w$)**를 추가로 출력하도록 변형해야 한다.

이미지의 왼쪽 상단을 (00) 우측 하단을 (11)로 둔다.  
    $b_x b_y$: 경계 상자 **중심**의 좌표.  
    $b_h b_w$: 경계 상자의 **높이**와 **너비** (전체 이미지 크기에 대한 비율).

### **3. 목표 레이블 $y$의 정의 (Target Label)**
지도 학습을 위한 목표 레이블 벡터 $y$는 다음과 같은 요소들을 포함한다.  
    $p_c$: 물체가 존재하는지 여부 (존재하면 1 배경이면 0).  
    $b_x b_y b_h b_w$: 경계 상자의 좌표 및 크기.  
    $c_1 c_2 c_3$: 각 클래스(보행자 자동차 오토바이 등)에 속할 확률.

**물체가 있는 경우 ($p_c=1$):**  
모든 좌표와 클래스 정보가 유효하다.

**물체가 없는 경우 ($p_c=0$):**  
나머지 요소($b_x... c_3$)들은 Don't care이 되며 신경망 훈련 시 고려할 필요가 없다.

### **4. Loss Function 설정**
신경망이 예측한 값($\hat{y}$)과 실제 값($y$)의 차이를 계산할 때 $p_c$의 값에 따라 다르게 계산한다.

**$y_1(p_c) = 1$일 때:**  
물체가 존재하므로 위치 좌표와 클래스 등 모든 8개 요소의 차이(제곱 오차 합 등)를 계산하여 손실에 반영한다.

**$y_1(p_c) = 0$일 때:**  
물체가 없으므로 $p_c$에 대한 예측 정확도만 중요하며 나머지 요소들의 오차는 신경 쓰지 않는다.
실제 구현에서는 클래스 예측에는 Log Likelihood를 좌표 예측에는 Squared Error 등을 혼합하여 사용할 수 있다.

단순히 물체를 분류하는 것을 넘어 신경망이 Real number 값을 출력하여 물체의 위치까지 파악.


## C4W3L02 Landmark Detection
단순히 물체의 Bounding Box를 찾는 것을 넘어 이미지 내의 중요한 **특징점(Landmark)**의 정확한 좌표를 찾아내는 방법


### **1. 특징점(Landmark) 검출의 기본 개념**
이전 강의에서는 물체의 위치를 Bounding Box의 네 가지 값($b_x b_y b_h b_w$)으로 표현했지만 신경망은 이미지 내의 중요한 특정 지점들의 **XY 좌표**를 직접 출력할 수도 있다.
>얼굴 인식 시스템을 만들 때 눈꼬리의 위치와 같은 구체적인 지점을 $l_x l_y$ 좌표로 정의하여 신경망의 출력층에서 이 값들을 얻어낼 수 있다.  
만약 두 눈의 네 군데 모서리를 모두 찾고 싶다면 신경망이 첫 번째 점부터 네 번째 점까지 각각의 좌표($l_{1x} l_{1y}$ ~ $l_{4x} l_{4y}$)를 출력하도록 수정하면 된다.

### **2. 다양한 특징점의 활용과 출력 유닛 구성**
특징점은 눈뿐만 아니라 입 모양, 코, 턱의 윤곽선 등을 따라 여러 개를 설정할 수 있다.
가령 얼굴 전체에 **64개의 특징점**을 설정한다고 가정하면 신경망의 출력 구조는 다음과 같이 설계됩니다.

첫 번째 유닛: 얼굴이 있는지 없는지를 판별 (0 또는 1).  
나머지 유닛: 64개의 특징점에 대한 XY 좌표들 ($l_{1x} l_{1y}$ ... $l_{64x} l_{64y}$).

총 출력 유닛의 개수는 $1 + (64 \times 2) = 129$개

### **3. 응용 분야: AR 필터와 그래픽 효과**
이러한 특징점 검출 기술은 Snapchat과 같은 앱에서 **AR 필터**를 구현하는 데 핵심적인 역할을 한다.

얼굴의 특징점 위치를 정확히 알면 얼굴 위에 왕관을 씌우거나 얼굴 모양을 변형시키는 등의 컴퓨터 그래픽 효과를 적용할 수 있다.

이를 훈련하기 위해서는 누군가가 직접 모든 특징점의 좌표를 찍어놓은 Label 훈련 세트가 필요하다.

### **4. Pose Detection와 레이블의 일관성**
이 기술은 얼굴뿐만 아니라 사람의 **자세 감지**에도 활용된다. 가슴 중앙 어깨 팔꿈치 손목 등을 특징점으로 정의하여 사람의 자세를 인식할 수 있다.

특징점의 순서(Identity)는 모든 이미지 데이터에서 **항상 동일**해야 한다.
>'특징점 1'이 왼쪽 눈의 왼쪽 모서리라면 다른 모든 이미지에서도 '특징점 1'은 반드시 같은 부위여야 한다.
충분히 큰 데이터 세트에 대해 일관된 순서로 레이블링을 한다면 신경망은 이러한 특징점들을 모두 출력해낼 수 있다.


## C4W3L03 Object Detection

### **1. Training Set Construction**
자동차 감지 알고리즘을 만들기 위해서는 먼저 레이블링된 훈련 세트(x y)를 구축해야 한다.
이때 **자동차가 이미지의 대부분을 차지하도록 근접하게 잘린 이미지**를 x로 사용한다.
이 데이터를 이용해 입력된 이미지가 자동차인지 아닌지(0 또는 1)를 판별하도록 합성곱 신경망을 훈련시킨다.

### **2. Sliding Window Detection의 기본 원리**
훈련된 신경망을 테스트 이미지에 적용할 때 특정 크기의 사각형 영역을 설정하여 시작한다.

이미지의 일부분인 이 작은 사각형 영역을 신경망에 입력하여 자동차 유무를 예측한다.
윈도를 옆으로 조금씩 이동시키며 이미지의 모든 위치에 대해 반복적으로 신경망을 통과시킨다.

### **3. 다양한 윈도 크기 적용**
작은 윈도로 전체 이미지를 훑은 후 이번에는 **더 큰 윈도**를 사용하여 같은 과정을 반복한다.

윈도의 크기를 바꿔가며 전체 이미지를 스캔함으로써 이미지 어딘가에 있는 자동차가 해당 윈도 안에 들어왔을 때 신경망이 '1'을 출력하여 감지하게 됩니다.

### **4. 슬라이딩 윈도의 단점: Computational Cost**
이 방식의 가장 큰 문제는 **계산 비용이 매우 높다**는 것이다.
이미지에서 수많은 영역을 잘라내어 각각 독립적으로 신경망을 통과시켜야 하기 때문이다.

**트레이드오프(Trade-off):**   
    윈도를 띄엄띄엄 이동시키면 계산 횟수는 줄지만 정확도가 떨어져 물체를 놓칠 수 있다.  
    매우 촘촘하게 이동시키면 정확도는 높아지지만 계산량이 폭증하여 속도가 너무 느려진다.




슬라이딩 윈도 방식을 **합성곱적으로 구현**하여 계산 효율을 획기적으로 높이는 방법이 존재한다.



## C4W3L04 Convolutional Implementation Sliding Windows
계산 비용이 높은 기존의 슬라이딩 윈도 방식을 합성곱 신경망을 통해 효율적으로 구현하는 방법을 설명한다.

### **1. Fully Connected Layer을 합성곱 층으로 변환**
**배경:**  
슬라이딩 윈도 알고리즘은 계산 속도가 너무 느리다는 단점이 있다. 이를 해결하기 위해 먼저 신경망의 완전 연결 층을 합성곱 층으로 바꾸는 방법을 알아야 한다.

**기존 구조:**  
 $14 \times 14 \times 3$ 이미지를 입력받아 합성곱과 풀링을 거쳐 $5 \times 5 \times 16$ 크기가 된 후 이를 400개의 유닛을 가진 완전 연결 층으로 펼치고 최종적으로 4개의 클래스(보행자 자동차 오토바이 배경)를 출력하는 소프트맥스 층으로 연결되는 구조가 있다.

**변환 방법:** 
    평범한 노드 집합 대신 필터를 사용하여 차원을 유지한다.  
    $5 \times 5 \times 16$ 크기의 볼륨을 400개의 $5 \times 5$ 필터로 합성곱 연산을 수행하면 $1 \times 1 \times 400$ 볼륨이 된다. 이는 수학적으로 기존의 완전 연결 층과 동일한다.  
    그다음 400개 유닛의 층과 최종 소프트맥스 층 역시 $1 \times 1$ 필터를 사용하는 합성곱 층으로 구현하여 최종적으로 $1 \times 1 \times 4$ 볼륨의 출력을 얻는다.

### **2. 슬라이딩 윈도의 비효율성 (기존 방식)**
테스트 이미지가 학습 이미지($14 \times 14$)보다 큰 $16 \times 16$이라고 가정할 때 기존 방식은 이미지를 잘라내어 신경망을 4번 각각 통과시켜야 한다.
이 방식은 겹치는 영역이 많음에도 불구하고 매번 독립적인 연산을 수행하므로 계산이 중복되어 매우 비효율적이다.

### **3. 합성곱을 이용한 슬라이딩 윈도 구현**
**계산 공유:**  
앞서 변환한 '완전 연결 층이 없는' 합성곱 신경망에 $16 \times 16$ 이미지를 통째로 입력한다.

**결과:**  
입력 이미지가 커졌기 때문에 최종 출력은 $1 \times 1 \times 4$가 아니라 $2 \times 2 \times 4$ 볼륨이 된다.  
    출력의 좌측 상단($1 \times 1$)은 원본 이미지의 좌측 상단 $14 \times 14$ 영역에 대한 예측 결과와 같다.
    마찬가지로 출력의 나머지 부분들도 각각 대응되는 슬라이딩 윈도 위치의 예측 결과를 나타낸다.

**장점:**  
4번의 독립적인 전파 과정을 거치는 대신 큰 이미지에 대해 한 번의 Forward pass만 수행하면 모든 영역에 대한 예측값(여기서는 4개 영역)을 동시에 얻을 수 있다. 이는 공통된 영역의 계산을 공유하므로 훨씬 효율적이다.

### **4. 결론 및 한계**
더 큰 이미지(예: $28 \times 28$)에 대해서도 동일한 방식을 적용하면 한 번의 연산으로 $8 \times 8 \times 4$와 같은 전체 예측 맵을 얻을 수 있다.
이 방식은 자동차의 존재 여부를 빠르게 파악할 수 있게 해주지만 Bounding Box의 위치가 아주 정밀하지 않을 수 있다는 단점이 있다. 


## C4W3L06 Intersection Over Union

### **1. IoU(Intersection Over Union)의 정의와 계산**
물체 감지 알고리즘이 얼마나 잘 작동하는지 평가하기 위해 '합집합 위의 교집합(IoU)' 함수를 사용한다.
IoU는 **교집합의 크기(두 경계 상자가 겹치는 영역)를 합집합의 크기(두 경계 상자가 차지하는 전체 영역)로 나눈 값**으로 계산한다.
즉 알고리즘이 예측한 보라색 경계 상자와 실제 정답인 빨간색 경계 상자가 얼마나 겹치는지를 수치화한 것이다.

### **2. 정확도 판단의 기준 (임계값)**
예측값과 실제 값이 완벽하게 일치하면 IoU는 1이 됩니다.
컴퓨터 비전 분야의 관례상 **IoU가 0.5 이상이면 올바르게 감지**한 것으로 판단한다.

0.5라는 수치는 절대적인 이론적 근거가 있는 것은 아니며 더 엄격한 평가를 원한다면 0.6이나 0.7과 같은 더 높은 기준을 사용할 수도 있다. 단 0.5 미만으로 기준을 낮추는 경우는 거의 없다.

### **3. IoU의 일반적 의미와 활용**
IoU는 단순히 정답 여부를 판별하는 것을 넘어 두 경계 상자가 서로 얼마나 유사한지(얼마나 겹치는지)를 측정하는 척도이다.
이 개념은 물체 감지 알고리즘의 성능 평가뿐만 아니라 성능을 향상시키는 또 다른 도구인 **'Non-max suppression'** 를 구현할 때 다시 사용된다.

## C4W3L07 Nonmax Suppression

### **1. 문제 정의: 중복 감지 현상**
물체 감지 알고리즘의 문제 중 하나는 **동일한 물체를 여러 번 감지**할 수 있다는 점이다.  
>19x19 격자에서 자동차의 중심점은 이론적으로 하나의 셀에만 할당되어야 한다. 하지만 실제로는 중심점 주변의 인접한 셀들도 해당 물체가 자기 안에 있다고 판단하여 하나의 자동차에 대해 여러 개의 Bounding Box를 예측하게 된다.

### **2. Non-max Suppression의 개념**
비-최댓값 억제는 알고리즘이 감지된 결과들을 정리하여 **각 물체당 하나의 감지 결과만 남도록 보장**하는 기법이다.
이름의 의미는 확률의 Max을 가진 것을 남기고 Non-max 것들을 억제한다는 뜻이다.

### **3. 작동 원리 (예시)**
**1단계:**  
감지된 모든 경계 상자 중 **가장 높은 확률($P_c$)을 가진 박스**를 선택하여 예측 결과로 확정한다 (예: 0.9).

**2단계:**  
확정된 박스와 많이 겹치는(즉 **IOU가 높은**) 나머지 박스들을 찾아 제거(억제)한다. 이는 동일한 물체를 중복해서 찾은 박스들을 지우는 과정이다.

**3단계:**  
남아있는 박스 중에서 다시 가장 높은 확률을 가진 박스를 선택하고 그와 겹치는 박스들을 제거하는 과정을 반복한다. 이 과정을 통해 모든 박스가 처리될 때까지 진행한다.

### **4. 알고리즘 구현 상세**
실제 구현 시에는 먼저 임계값(예: 0.6) 이하의 낮은 확률을 가진 모든 경계 상자를 우선적으로 버린다.

그 후 다음 과정을 반복한다:
1.  남은 박스 중 가장 확률($P_c$)이 높은 것을 선택해 예측값으로 출력한다.
2.  선택된 박스와 높은 IOU를 가진(많이 겹치는) 박스들을 모두 버립니다.

만약 보행자 자동차 오토바이 등 **여러 클래스를 감지**해야 한다면 **각 클래스별로 독립적으로** 비-최댓값 억제를 수행해야 한다.



## C4W3L08 Anchor Boxes
하나의 rid Cell에서 여러 물체를 동시에 감지해야 할 때 발생하는 문제를 해결하기 위한 **'Anchor Boxes'** 의 개념과 적용 방법을 다룬다.

### **1. 문제 제기와 앵커 박스의 도입**
**문제점:**  
기존의 물체 감지 알고리즘은 하나의 격자 셀이 오직 하나의 물체만 감지할 수 있다는 한계가 있다. 예를 들어 보행자와 자동차의 중심점이 거의 같은 위치에 있어 동일한 격자 셀에 배정되어야 한다면 기존 방식으로는 둘 중 하나만 선택해야 하는 문제가 발생한다.

**해결책:**  
미리 정의된 서로 다른 모양의 '앵커 박스'를 여러 개 사용하여 하나의 셀에서 여러 예측값을 동시에 출력하도록 한다.

### **2. 출력 벡터(y)의 구조 변화**
기존에는 격자 셀 하나당 하나의 벡터($p_c b_x b_y b_h b_w c_1 c_2 c_3$)를 출력했지만 앵커 박스를 도입하면 이 벡터를 앵커 박스의 개수만큼 반복해서 쌓습니다.
예를 들어 2개의 앵커 박스를 사용하고 3개의 클래스(보행자 차 오토바이)를 분류한다면 출력 벡터 $y$의 차원은 기존 8차원에서 16차원(2개의 앵커 박스 × 8개의 값)으로 늘어납니다.

**앵커 박스 1:**  
    보행자처럼 세로로 긴 물체를 담당 (첫 8개 값)  
**앵커 박스 2:**  
    자동차처럼 가로로 넓은 물체를 담당 (나중 8개 값).

### **3. 물체 배정 규칙 (Assigning Objects)**
훈련 시 각 물체는 두 가지 기준에 따라 배정된다.

1.  물체의 중심점이 위치한 **격자 셀**.
2.  물체의 실제 Ground Truth와 가장 높은 **IOU(Intersection over Union 교차법)**를 가지는 **앵커 박스**.

물체는 단순히 '격자 셀'에만 배정되는 것이 아니라 **(격자 셀 앵커 박스)의 쌍**에 배정된다.

### **4. 구체적인 레이블링 예시**
**두 물체가 겹친 경우:**  
보행자와 자동차가 같은 셀에 있다면 보행자 데이터는 모양이 비슷한 '앵커 박스 1' 위치에 자동차 데이터는 '앵커 박스 2' 위치에 입력되어 하나의 벡터 안에 두 물체 정보가 모두 담깁니다.

**한 물체만 있는 경우:**  
만약 자동차만 있고 보행자가 없다면 자동차는 '앵커 박스 2' 위치에 정보를 입력하고 보행자가 담당하는 '앵커 박스 1' 위치의 $p_c$(물체 존재 확률)는 0이 됩니다(나머지 값은 무시).

### **5. 한계점과 알고리즘의 이점**
**한계:**  
앵커 박스가 2개인데 3개의 물체가 한 셀에 겹치거나 2개의 물체가 모두 같은 모양(같은 앵커 박스)을 가질 경우에는 이 알고리즘만으로는 처리가 어렵다. 하지만 격자가 촘촘할수록(예: 19×19) 이런 일이 발생할 확률은 매우 낮다.

**이점:**  
단순히 겹친 물체를 구별하는 것을 넘어 학습 알고리즘이 특정 모양(길쭉한 물체 vs 넓은 물체)에 **Specialize**되도록 돕는다.

**6. 앵커 박스 선정 방법**
일반적으로 감지하고자 하는 물체들의 형태를 고려하여 수동으로 5~10개를 고를 수 있다.
더 발전된 방식(YOLO 논문 등)에서는 **K-means 알고리즘**을 사용하여 데이터셋에 있는 실제 물체들의 모양을 그룹화하고 이를 대표하는 모양을 자동으로 앵커 박스로 선정한다.



**ai:** 

이 개념은 주차장 시스템에 비유할 수 있다. 기존 방식이 **'구역 하나당 차량 한 대'** 만 주차할 수 있어서 오토바이와 버스가 같은 구역에 오면 문제가 생겼다면 앵커 박스 방식은 각 구역을 **'소형차 전용 칸'과 '대형 화물차 전용 칸'** 으로 미리 나누어 놓은 것과 같습니다. 이렇게 하면 같은 구역에 오토바이(소형)와 버스(대형)가 동시에 도착해도 각각 맞는 칸에 주차(감지)할 수 있게 됩니다.

## C4W3L09 YOLO Algorithm
물체 감지의 요소들을 결합하여 YOLO 알고리즘을 실제로 구성하는 방법

### **1. Training Set 구성 및 출력 차원 설정**
**클래스 정의:**  
보행자 자동차 오토바이 3가지 클래스를 감지한다고 가정한다.

**격자 및 앵커 박스:**  
$3 \times 3$ 격자(Grid)와 2개의 앵커 박스(Anchor Box)를 사용한다고 가정한다.

**출력값($y$)의 차원 계산:**  
    각 격자 셀은 2개의 앵커 박스에 대한 정보를 담습니다.  
    각 앵커 박스는 8개의 값을 가집니다: $P_c$ (물체 존재 확률) 경계 상자 좌표 4개($b_x b_y b_h b_w$) 클래스 확률 3개($c_1 c_2 c_3$).  
    따라서 전체 출력 부피의 차원은 $3 \times 3 \times (2 \times 8)$ 즉 **$3 \times 3 \times 16$** 이 된다.

**표적 벡터($y$) 생성:**  
    물체가 없는 격자 셀: 두 앵커 박스 모두 $P_c=0$이 되며 나머지 값들은 신경 쓰지 않는다(Don't care).

### **2. 앵커 박스 할당 및 일반화**
**앵커 박스 선택 기준:**  
실제 물체의 경계 상자 모양과 더 높은 IOU를 가지는 앵커 박스에 물체를 할당한다.
> 자동차가 앵커 박스 2와 모양이 비슷하다면 해당 격자의 앵커 박스 1은 $P_c=0$ 앵커 박스 2는 $P_c=1$과 함께 구체적인 좌표 및 클래스 정보를 가진다.

**실제 적용:**  
강의 예시에서는 $3 \times 3$ 격자를 사용했으나 실제로는 $19 \times 19$와 같이 더 촘촘한 격자를 사용하며 앵커 박스도 5개 등을 사용하여 출력 차원이 $19 \times 19 \times 40$ ($5 \times 8$) 등으로 커질 수 있다.

### **3. 신경망의 Prediction 과정**
입력 이미지(예: $100 \times 100 \times 3$)를 신경망에 넣으면 훈련된 차원(예: $3 \times 3 \times 16$)의 볼륨이 출력된다.

**물체가 없는 셀:**  
신경망은 $P_c$에 낮은 확률(0에 가까운 값)을 출력하고 나머지 좌표나 클래스 값은 노이즈로 간주되어 무시된다.

**물체가 있는 셀:**  
해당 셀의 특정 앵커 박스 위치에서 정확한 경계 상자 좌표와 클래스 확률을 출력한다.

### **4. Non-max Suppression 적용**
신경망은 모든 격자 셀(9개)에 대해 앵커 박스 수(2개)만큼 즉 총 18개의 경계 상자를 예측한다.

**1단계:**  
확률($P_c$)이 낮은 예측들을 제거한다.  
**2단계:**  
각 클래스(보행자 자동차 오토바이)별로 독립적으로 비-최댓값 억제를 실행하여 중복된 박스를 제거하고 가장 정확한 박스만 남깁니다.
이 과정을 거치면 최종적으로 감지된 물체들만 남게 되며 이것이 YOLO 알고리즘의 전체적인 흐름이다.

## C4W3L10 Region Proposals


### **1. Sliding Window의 비효율성**
기존의 슬라이딩 윈도 방식은 이미지의 모든 영역에 대해 분류기를 실행한다.  
하지만 이미지의 많은 부분은 물체가 없는 단순한 배경(예: 검은색 부분)인 경우가 많아 아무것도 없는 영역까지 분류기를 돌리는 것은 비효율적이다.

### **2. R-CNN (Region with CNN)의 핵심 아이디어**
**개념:**  
모든 윈도를 검사하는 대신 물체가 있을 법한 **'Region'** 몇 개만 선택하여 그곳에만 분류기를 실행하자는 아이디어이다.

**작동 방식:** 
1.  **Segmentation Algorithm:**  
    이미지에서 색상이나 질감 등을 기반으로 물체라고 의심되는 영역(blob)들을 찾아낸다.
2.  **후보 영역 선정:**  
    약 2000개의 후보 영역(Region Proposals)을 생성한다. 이는 모든 위치를 훑는 것보다 훨씬 적은 횟수이다.
3.  **분류기 실행:**  
    선택된 영역에 대해서만 합성곱 신경망(CNN)을 실행하여 자동차 보행자 등을 판별한다.

### **3. R-CNN의 출력과 한계**
**출력:**  
각 지역에 대한 클래스 레이블(어떤 물체인지)뿐만 아니라 더 정확한 위치를 잡기 위해 경계 상자 좌표($b_x b_y b_h b_w$)까지 조정하여 출력한다.

**단점:**  
초기 R-CNN 알고리즘은 2000개의 영역을 각각 처리해야 하므로 **속도가 상당히 느리다**는 단점이 있다.

### **4. 속도 개선 모델: Fast R-CNN & Faster R-CNN**
느린 R-CNN을 개선하기 위해 후속 연구들이 진행되었다.

**Fast R-CNN (Ross Girshick):**  
    기존 R-CNN에 **합성곱 슬라이딩 윈도(Convolutional Sliding Window)** 구현 방식을 적용하여 분류 단계를 가속화했습니다. 하지만 클러스터링은 여전히 느렸다.

**Faster R-CNN (Ren et al.):**  
    느린 분할 알고리즘 대신 **Neural Network**을 사용하여 클러스터링한다. 이를 통해 Fast R-CNN보다 속도를 더 높였다.

### **5. 결론 및 YOLO와의 비교**
Faster R-CNN조차도 **YOLO 알고리즘보다는 속도가 느린 편**이다.  
클러스터링과 분류가 분리된 2단계 방식(R-CNN 계열)보다는 YOLO처럼 한 번에 처리하는 방식을 선호한다고한다.

