---
layout: post
title: "[DeepLearning.AI] Course 5추가 of the Deep Learning Specialization"
date: 2026-01-15 12:50
categories: MyStudy DeepLearning.AI
tags: DeepLearning.AI DeepLearning
math: true
---

[Sequence Models (Course 5 of the Deep Learning Specialization)](https://www.youtube.com/watch?v=_i3aqgKVNQI&list=PLkDaE6sCZn6F6wUI9tvS_Gw1vaFAx6rd6)


음성 인식을 위해 **Attention model**을 사용하거나 **CTC(Connectionist Temporal Classification) 비용 함수**를 활용할 수 있다.

실제 구현 시에는 단방향 RNN보다는 주로 **양방향 LSTM(Bidirectional LSTM)이나 GRU**와 같은 깊은 모델을 사용한다.

음성 인식에서는 입력 타임 스텝(예: 1000개)이 출력 캐릭터 수(예: 19개)보다 훨씬 많은 것이 일반적이다.

**CTC 비용 함수**는 이를 해결하기 위해 **특수 문자 blank**와 **반복된 문자를 하나로 합치는 규칙**을 사용한다.
>"TTT_h_eee"라는 출력이 있다면 blank(_)가 없는 구간의 반복된 글자를 합쳐 "The"로 변환한다.

이 방식을 통해 수천 개의 출력 값을 가지는 신경망이 훨씬 짧은 텍스트 스크립트를 효과적으로 표현할 수 있다.

현재 프로덕션 수준의 음성 인식 시스템을 구축하는 것은 매우 방대한 데이터가 필요한 큰 작업이다.





>blank를 어차피 합치는데 왜필요한가
{: .prompt-warning }

**연속된 같은 문자**:  
CTC의 핵심 규칙은 연속된 같은 문자를 하나로 합치는 것이다. 만약 blank가 없다면 'apple'이나 'hello'처럼 같은 문자가 연달아 나오는 단어를 인식할 수 없다.

blank가 없을 때: ppple → ple (p가 하나로 합쳐짐)  
blank가 있을 때: pp-pple → p-ple → pple (중간에 blank가 있어 p가 합쳐지지 않음)

blank는 똑같은 소리가 반복될 때 이를 별개의 문자로 떼어놓는 구분자 역할을 한다.



**Time Step 정렬**:  
음성 데이터는 텍스트에 비해 데이터의 길이가 압도적으로 길다. 1초의 짧은 말소리도 수백 개의 프레임으로 쪼개지는데 모델은 모든 프레임마다 반드시 결과값을 내놓아야 한다.

의미 없는 구간 처리: 숨소리, 침묵, 혹은 소리와 소리 사이의 모호한 전이 구간에서 억지로 글자를 출력할 수 없으므로 blank를 채워 넣다.

길이 조절: 수백 개의 출력값 중 실제 글자가 있는 위치를 제외한 나머지를 blank로 채움으로써 입력 음성과 출력 텍스트 사이의 길이 차이를 수학적으로 해결한다.


>연음같은 거 깔끔하게 안끊기는거 같은데 어떻게 나누는지
{: .prompt-warning }


CTC는 음성 데이터의 경계를 물리적으로 나누지 않고 **모든 시간 단위에서 예측된 결과를 사후에 하나로 뭉치는 'Many-to-One' 매핑 방식**으로 연음 문제를 해결한다.

CTC는 사람이 일일이 경계를 나누는 대신 가능한 모든 경우의 수를 수학적으로 합산하는 방식이다.

강제 분할 과정을 생략한다:  
학습을 위해 모든 음성 데이터의 프레임마다 정답 라벨을 일일이 달아줘야 했다. 하지만 CTC는 이 번거로운 작업을 포기하고 대신 모델이 스스로 전체 시퀀스 안에서 정답을 찾아가도록 내버려 두는 방식을 사용한다.

모든 경로 허용:  
중복 제거와 Blank규칙을 사용하여 
입력 시퀀스의 각 타임스텝마다 발생할 수 있는 모든 문자의 확률을 더하는 방식으로 계산한다.

연음이 발생하여 한 문자가 끝나고 다음 문자로 넘어가는 시점이 깔끔하지 않을 때 blank로 넘긴다


CTC는 사람이 직접 나누기 어려운 모호한 경계를 모든 가능한 정렬의 확률 합이라는 개념으로 우회하여 해결힌다. 어떻게 나누냐는 질문에 대한 CTC의 답은 직접 나누지 않고 발생 가능한 모든 나눗셈의 경우의 수를 다 고려한다가 된다.


>space는 왜 필요한가
{: .prompt-warning }


**space는 단어와 단어를 구분하는 실제 텍스트 구성 요소**이기 때문에 필요하다.

**blank**는 모델이 연산을 위해 사용하는 가상의 기호일 뿐 최종 결과물에서는 사라진다. 반면 **space**는 우리가 읽는 문장에서 'the'와 'quick'을 'thequick'이 아닌 'the quick'으로 나누어주는 **실제 문자**이다. 모델 입장에서는 'a', 'b', 'c'와 마찬가지로 학습해야 할 하나의 Class이다.




**Blank ( _ ):**  
기술적 도구이다. 같은 문자가 연속될 때 이들을 합치지 않게 막아주거나 음성 신호가 없는 구간을 채우는 역할을 한다. 결과 출력 시에는 모두 제거된다.  
**Space ( ):**  
언어적 도구이다. 단어 사이의 공백을 나타내며 결과 출력 시에도 문장에 그대로 남아 띄어쓰기를 완성한다.


> **TTT _ _ HHH EEE _ _ (space) (space) _ _ QQQ UUU...**

이 예측 결과에서 중복을 제거하고 blank( _ )를 지우면 다음과 같이 남는다.

> **T H E (space) Q U I C K**

만약 여기서 space가 없다면 모델은 단어 사이의 경계를 알 수 없어 모든 글자를 붙여서 출력하게 된다. **space는 사람이 읽을 수 있는 문장을 만들기 위한 필수 문자**이다.



>일본어 중국어는 공백이 없다?
{: .prompt-warning }





영어와 한국어는 띄어쓰기가 의미 전달과 가독성에 필수적이므로 모델에게 space라는 글자를 출력하도록 가르칩니다. 반면 일본어와 중국어는 단어 사이를 띄우지 않는 것이 기본 규칙이다.

이들 언어를 학습시킬 때는 모델이 내뱉어야 할 정답지에서 space를 아예 빼버린다. 모델은 단순히 글자와 글자를 이어서 출력하는 법만 배운다.  


blank는 띄어쓰기가 아니라 중복을 막아준다

**중복 글자 보존:**  
중국어의 妈妈(māma)처럼 같은 소리가 연속될 때 blank가 없다면 CTC의 합치기 규칙 때문에 'ma' 한 글자로 인식되는 오류가 발생한다. 'ma - blank - ma' 형태의 경로가 있어야만 두 글자를 온전히 보존할 수 있다.

음성은 긴데 출력할 글자는 적은 물리적 차이를 메꾸기 위해 blank는 반드시 필요한다.


* **영어/한국어:** [문자들] + [space 기호] + [blank 토큰]
* **일본어/중국어:** [문자들] + [blank 토큰] (space 제외)

space는 **우리가 읽기 편하려고 넣는 글자**일 뿐


일본어, 중국어는 언어 자체가 띄어쓰기를 안 쓰기 때문에 모델이 space를 출력할 필요가 없다.
