---
layout: post
title: "[CS231n] 3. Loss Functions and Optimization"
date: 2026-01-13 09:50
categories: MyStudy CS231n
tags: cv CS231n
math: true
---

강의 주소: <br>
[CS231n Lecture 3. Loss Functions and Optimization](https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk&index=3)

자료: <br>
[깃허브 강의 자료](https://github.com/visionNoob/CS231N_17_KOR_SUB?tab=readme-ov-file)



### 1. 도입 및 지난 강의 복습 
지난 시간의 이미지 분류 도전 과제와 선형 분류기(Linear Classifier)를 복습했습니다. 선형 분류기는 이미지의 픽셀 값을 가중치 행렬 $W$와 곱해 각 클래스에 대한 점수(Score)를 산출하며 이는 각 클래스에 대한 **템플릿을 학습하는 것**으로 해석될 수 있습니다.

### 2. 손실 함수 (Loss Functions) 
**정의:**  
임의의 가중치 $W$가 얼마나 "나쁜지(Badness)"를 정량화하는 함수입니다.
**목표:**  
훈련 데이터셋의 예측 점수와 실제 정답 레이블(Target)을 비교하여 예측이 틀릴수록 높은 값을 가집니다. 최적화 과정은 이 손실 함수의 값을 최소화하는 $W$를 찾는 과정입니다.

### 3. 멀티클래스 SVM 손실 (Multiclass SVM Loss) 
**개념:**  
정답 클래스의 점수가 오답 클래스의 점수보다 일정한 마진(보통 1) 이상으로 높기를 기대합니다.
**힌지 손실 (Hinge Loss):**  
정답 점수가 마진을 확보하면 손실이 0이 되고 그렇지 않으면 선형적으로 손실이 증가하는 형태를 가집니다.
**특징:** 
    초기 $W$가 작아 모든 점수가 0에 가까우면 손실 값은 대략 '클래스 수 - 1'이 됩니다 (디버깅 시 유용한 지표).
    평균(Mean)을 사용하든 합계(Sum)를 사용하든 결과적인 분류기의 성능에는 큰 차이가 없습니다.

### 4. 규제화 (Regularization) 
**필요성:**  
훈련 데이터에만 완벽하게 맞추려다 보면 모델이 복잡해져 새로운 데이터(테스트 데이터)에 대한 성능이 떨어지는 **과적합(Overfitting)**이 발생할 수 있습니다.
**오캄의 면도날 (Occam's Razor):**  
여러 가설 중 가장 간단한 것을 선호해야 한다는 원칙에 따라 모델에 "단순함"을 강요하기 위해 손실 함수에 규제 항을 추가합니다.
**L2 규제 (Weight Decay):**  
가중치의 유클리드 노름을 페널티로 부여하여 가중치가 특정 픽셀에 치우치지 않고 골고루 퍼지게 만듭니다. 이는 모델이 더 견고(Robust)해지도록 돕습니다.

### 5. 소프트맥스 분류기 (Softmax Classifier) 
**개념:**  
선형 분류기의 점수를 확률로 해석합니다.
**소프트맥스 함수:**  
점수를 지수화하여 양수로 만든 뒤 전체 합으로 나누어 0에서 1 사이의 확률값(합계 1)으로 변환합니다.
**손실 계산:**  
정답 클래스의 확률에 마이너스 로그($-\log$)를 취합니다. 즉 정답 확률이 1에 가까울수록 손실은 0에 가까워집니다.
**SVM과의 차이:**  
SVM은 마진만 확보되면 더 이상 개선하려 하지 않지만 **소프트맥스는 끊임없이 정답 확률을 1로 높이려고 시도**합니다.

### 6. 최적화 (Optimization) 
**비유:**  
눈을 가린 채 산골짜기에서 가장 낮은 지점을 찾아 내려가는 것과 같습니다.
**경사 하강법 (Gradient Descent):**  
현재 위치에서 경사(Gradient)의 반대 방향으로 조금씩 이동하여 손실 함수의 최솟값을 찾습니다.
**기울기 계산:**  
    **수치적 기울기 (Numerical Gradient):**  
    직접 값을 조금씩 바꿔보며 계산하며 느리고 부정확하지만 구현이 쉬워 **디버깅(Gradient Check)**용으로 사용합니다.
    **해석적 기울기 (Analytic Gradient):**  
    미분을 통해 공식을 도출하여 한 번에 계산하며 정확하고 빠르기 때문에 실제 학습에 사용합니다.
**확률적 경사 하강법 (SGD):**  
전체 데이터셋이 너무 클 경우 데이터의 일부인 **미니 배치(Mini-batch)**만 샘플링하여 기울기를 추정하고 업데이트합니다.

### 7. 이미지 특징량 (Image Features) 
**과거의 방식:**  
픽셀을 직접 넣는 대신 색상 히스토그램(Color Histogram) HOG(방향성 경사 히스토그램) Bag of Words 등 수동으로 설계된 특징 추출기를 거친 뒤 선형 분류기를 학습시켰습니다.
**딥러닝과의 차이:**  
딥러닝(CNN)은 특징 추출기를 사람이 설계하지 않고 **데이터로부터 특징 표현을 직접 학습**한다는 점이 핵심적인 차이입니다.


