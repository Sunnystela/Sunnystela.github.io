---
layout: post
title: "[CS231n] 10. Recurrent Neural Networks"
date: 2026-02-11 09:50
categories: MyStudy CS231n
tags: cv CS231n
math: true
---

강의 주소: <br>
[CS231n Lecture 10. Recurrent Neural Networks](https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=10)

자료: <br>
[깃허브 강의 자료](https://github.com/visionNoob/CS231N_17_KOR_SUB?tab=readme-ov-file)



### 1. 지난 강의 복습: CNN 아키텍처와 그래디언트 흐름

**CNN 아키텍처 흐름:**  
2012년 AlexNet(9개 레이어)이 딥러닝 혁명을 시작했고 2014년 VGG(16/19 레이어)와 GoogLeNet(22 레이어)으로 깊어졌다.

**Batch Normalization 이전의 어려움:**  
2014년 당시에는 배치 정규화가 없어서 깊은 모델 학습이 어려웠다. VGG는 11개 레이어를 먼저 학습시킨 뒤 중간에 레이어를 추가하는 방식을 썼고 GoogLeNet은 보조 분류기를 통해 중간 레이어에 그래디언트를 주입하는 해킹에 가까운 방법을 사용했다.

**ResNet과 그래디언트 슈퍼하이웨이:**   
2015년 ResNet은 **Residual Connection**을 도입했다. 이는 덧셈 연산 덕분에 역전파 시 그래디언트가 분산되어 네트워크의 끝에서 처음까지 막힘없이 흐르는 역할을 한다. 덕분에 수백 개의 레이어도 쉽게 학습할 수 있게 되었다. 이 **Gradient Flow** 개념은 오늘 다룰 RNN에서도 매우 중요하다.

**파라미터 효율성:**  
AlexNet과 VGG는 파라미터 대부분이 마지막 Fully Connected 레이어에 집중되어 있다(AlexNet의 경우 약 6200만 개 중 대부분). 반면 ResNet이나 GoogLeNet은 마지막에 **Global Average Pooling**을 사용하여 파라미터 수를 획기적으로 줄였다.

### 2. 순환 신경망(RNN)의 개요

**기존 신경망의 한계:**  
지금까지 배운 Vanilla Neural Network나 CNN은 고정된 크기의 입력(이미지 등)을 받아 고정된 크기의 출력(클래스 점수)을 내는 **One-to-One** 방식이었다.

**RNN의 유연성:**  
RNN은 가변적인 길이의 시퀀스 데이터를 처리할 수 있다.
    
**One-to-Many:**  
    고정 입력(이미지) -> 가변 출력(캡션 생성).
    
**Many-to-One:**  
    가변 입력(텍스트) -> 고정 출력(감성 분석).
    
**Many-to-Many:**  
    가변 입력(영어 문장) -> 가변 출력(프랑스어 번역). 입력과 출력의 길이가 다를 수 있다.
    
**Video Classification:**  
    비디오 프레임마다 분류를 수행하는 경우.

**고정 입력에서의 활용:**  
숫자가 있는 이미지를 입력받아도 RNN을 사용해 이미지의 여러 부분을 순차적으로 훑어보며 숫자를 분류하거나 캔버스에 그림을 조금씩 그려 나가는 생성 모델 등에도 활용된다.

### 3. Vanilla RNN의 작동 원리

**기본 구조:**  
RNN 셀은 입력($x_t$)과 **이전 시점의 은닉 상태(Hidden State, $h_{t-1}$)**를 받아 **다음 은닉 상태($h_t$)**를 갱신한다. 이 과정은 매 Time step마다 반복된다.

**수식:**  
$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t)$. 이전 은닉 상태와 현재 입력을 각각 가중치와 곱한 뒤 더하고 tanh 비선형 함수를 통과시킵니다.

**가중치 공유:**  
중요한 점은 모든 시간 단계에서 **동일한 가중치 행렬 $W$를 재사용**한다는 것이다.

**Computational Graph:**  
이를 시간 순서대로 펼쳐서 표현할 수 있다. 각 단계에서 출력($y_t$)을 내고 Loss을 계산할 수 있으며 전체 손실은 각 단계 손실의 합이 된다.

**Backpropagation:**  
모델 학습을 위해 전체 손실에 대한 가중치 $W$의 그래디언트를 구해야 한다. $W$는 모든 단계에서 사용되므로 각 단계에서 계산된 $W$에 대한 그래디언트들을 모두 더해 최종 업데이트를 수행한다. 이를 **Backpropagation Through Time (BPTT)**라고 한다.

### 4. RNN 활용 예시 1: 문자 단위 언어 모델 (Character-level Language Model)

**설정:**  
'hello'라는 단어를 학습한다고 가정할 때, 사전은 {h, e, l, o} 4개입니다. 각 문자는 **One-hot vector**로 표현된다.

**학습 과정:**
    1.  입력 'h'가 들어오면 다음 문자로 'e'를 예측해야 한다. 모델이 엉뚱한 예측을 하면 손실을 계산한다.
    2.  정답인 'e'를 다음 단계의 입력으로 넣고 다음 문자인 'l'을 예측하도록 한다.
    3.  이 과정을 시퀀스 끝까지 반복한다.

**Sampling 과정:**
    1.  모델에 초기 Seed을 주면, 모델은 다음 문자에 대한 Softmax scores를 출력한다.
    2.  가장 확률이 높은 문자를 선택하는 대신 **확률 분포에 따라 Sampling**을 한다. 이를 통해 모델 출력에 다양성을 줄 수 있다.
    3.  샘플링된 문자를 다시 다음 단계의 입력으로 넣어 문장을 계속 생성한다.

**Truncated Backpropagation Through Time:**  
위키피디아 전체와 같이 시퀀스가 매우 길 경우 전체를 다 보고 역전파를 하면 메모리 부족 및 속도 문제가 발생한다. 따라서 데이터를 **작은 청크로 잘라서** 순전파/역전파를 수행한다. 단 은닉 상태($h$)는 다음 청크로 계속 전달하여 문맥을 유지한다.


### 6. RNN 활용 예시 2: Image Captioning

**구조:**  
CNN으로 이미지를 처리하여 벡터를 추출하고 이 벡터를 RNN의 초기 상태로 넣거나 입력에 추가한다. RNN은 캡션(단어 시퀀스)을 생성한다.

**특징:**
    
`<START>` 토큰을 입력해 생성을 시작하고, 모델이 `<END>` 토큰을 샘플링하면 생성을 멈춘다.
    
Supervised Learning으로 CNN과 RNN을 동시에 학습시킨다.

**결과 및 한계:**  
"가방 위에 앉아 있는 고양이" 등을 잘 묘사하지만 학습 데이터에 없는 패턴은 제대로 인식하지 못하거나 데이터의 편향을 보일 수 있다.

### 7. Attention 메커니즘

**개념:**  
이미지를 하나의 벡터로 압축하는 대신 RNN이 매 단계마다 **이미지의 어느 부분을 볼지** 결정하게 한다.

**작동:**  
CNN은 위치별 특징 벡터를 출력하고 RNN은 매 단어 생성 시마다 이 위치들에 대한 분포(어텐션 맵)를 만듭니다. 

**Soft vs Hard:**  
모든 영역을 가중 합산해서 보는 **Soft Attention**(미분 가능)과 특정 위치만 딱 골라서 보는 **Hard Attention**(미분 불가능, 강화학습 필요)이 있다.

### 8. Visual Question Answering (VQA)

이미지와 질문을 입력받아 정답을 맞히는 문제다. RNN으로 질문을 벡터화하고 CNN으로 이미지를 벡터화한 뒤 두 벡터를 결합하여 정답을 예측한다.

### 9. RNN 학습의 문제점과 해결책

**다층 RNN (Multilayer RNN):**  
RNN도 층을 쌓을 수 있으며 보통 2~4층 정도를 사용한다.

**Vanilla RNN의 그래디언트 문제:**  
역전파 시 tanh와 가중치 행렬 $W$를 반복적으로 곱하게 된다.
    
**Exploding Gradients (기울기 폭발):**  
    $W$의 가장 큰 Singular value이 1보다 크면 그래디언트가 기하급수적으로 커진다.  
**해결책:**    
**Gradient Clipping** (그래디언트의 L2 Norm이 임계값을 넘으면 잘라냄)을 사용한다.
    
**Vanishing Gradients (기울기 소실):**  
    특이값이 1보다 작으면 그래디언트가 0으로 사라져 Long-term dependency을 학습하지 못한다. Vanilla RNN에서는 해결하기 어렵다.

### 10. LSTM (Long Short Term Memory)

**개요:**  
기울기 소실 문제를 해결하기 위해 1997년에 제안된 구조입니다.

**두 가지 상태:**  
**Hidden State ($h_t$)** 외에 **Cell State ($c_t$)**라는 내부 상태를 추가로 가진다.

**4개의 게이트 (i, f, o, g):**  
입력 $x_t$와 이전 은닉 상태 $h_{t-1}$을 받아 4개의 벡터를 계산한다.
    
**f (Forget gate):**  
    이전 기억을 얼마나 잊을지 결정 (Sigmoid).
    
**i (Input gate):**  
    새로운 정보를 얼마나 기록할지 결정 (Sigmoid).
    
**g (Gate gate):**  
    기록할 정보의 후보 값 (Tanh).
    
**o (Output gate):**  
    은닉 상태로 얼마나 내보낼지 결정 (Sigmoid).

**Cell State 업데이트:**  
$c_t = f \odot c_{t-1} + i \odot g$.
    
핵심은 행렬 곱셈이 아니라 **Element-wise multiplication**과 **덧셈**으로 업데이트가 일어난다

**그래디언트 슈퍼하이웨이:**  
역전파 시 덧셈 연산을 통해 그래디언트가 **Cell State를 타고 막힘없이 뒤로 흐를 수 있다**. 이는 ResNet의 원리와 유사하며 기울기 소실 문제를 효과적으로 방지한다.

### 11. GRU 및 결론

**GRU (Gated Recurrent Unit):**  
LSTM을 조금 더 단순화한 구조로 Cell State 없이 Hidden State만 사용하지만 비슷한 게이팅 메커니즘을 쓴다.

**구조 탐색:**  
구글 등에서 수많은 변형을 실험해 봤지만 LSTM이나 GRU보다 압도적으로 좋은 구조는 발견되지 않았다. 따라서 **LSTM을 기본으로 사용**하는 것이 좋다.

RNN은 가변 길이 데이터를 처리할 수 있어 강력하지만 기울기 소실 문제가 있다. 이를 해결하기 위해 아키텍처적으로 개선된 LSTM이나 GRU를 사용한다.

