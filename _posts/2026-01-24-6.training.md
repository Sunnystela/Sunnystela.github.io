---
layout: post
title: "[CS231n] 6. Training Neural Networks I"
date: 2026-01-24 09:50
categories: MyStudy CS231n
tags: cv CS231n
math: true
---

강의 주소: <br>
[CS231n Lecture 6. Training Neural Networks I](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=9)

자료: <br>
[깃허브 강의 자료](https://github.com/visionNoob/CS231N_17_KOR_SUB?tab=readme-ov-file)



### 1. 복습
이전 강의들에서는 Computational Graph를 통해 함수를 표현하는 법, 선형 레이어와 비선형성을 쌓아 신경망을 만드는 법, 그리고 공간적 구조를 보존하는 CNN에 대해 배웠다.

신경망 학습은 Mini-batch SGD을 통해 데이터를 샘플링하고, Forward 전파로 손실을 구한 뒤, Backprop로 그레디언트를 계산하여 파라미터를 업데이트하는 과정을 반복한다.

### 2. Activation Functions

**Sigmoid:**   
$1 / (1 + e^{-x})$ 형태로 입력을 사이로 압축한다. 과거에 뉴런의 발화율로 해석되어 자주 쓰였으나 현재는 잘 쓰이지 않는다.  
    **문제점 1 (Gradient Kill):**   
    입력이 매우 크거나 작으면 기울기가 0에 가까워져 역전파 시 그레디언트가 소실되어 더 이상 학습이 진행되지 않는다.  
    **문제점 2 (Not Zero-centered):**   
    출력이 항상 양수이므로 다음 레이어의 가중치에 대한 그레디언트가 모두 양수이거나 모두 음수가 된다. 이로 인해 가중치 업데이트가 지그재그 형태로 비효율적으로 일어난다.  
    **문제점 3:**   
    지수 함수 계산 비용이 든다.

**Tanh:**   
[-1, 1] 범위로 압축하여 **Zero-centered** 문제는 해결했지만 여전히 양 끝단에서 기울기가 사라지는 문제는 남아 있다.  

**ReLU (Rectified Linear Unit):**   
$f(x) = \max(0, x)$ 형태이다.  
    **장점:**   
    양수 영역에서 포화되지 않으며 계산이 매우 효율적이고 수렴 속도가 시그모이드/Tanh보다 약 6배 빠르다.  
    **단점:**   
    Zero-centered가 아니며 음수 영역에서는 그레디언트가 0이 되어 뉴런이 죽어버리는 **'Dead ReLU'** 현상이 발생할 수 있다 (가중치 초기화가 나쁘거나 학습률이 너무 높을 때 발생).

**ReLU 변형들:**  
    **Leaky ReLU:**   
    음수 영역에 약간의 기울기를 주어 뉴런이 죽는 것을 방지한다.  
    **PReLU:**   
    음수 영역의 기울기를 파라미터로 만들어 학습시킵니다.  
    **ELU:**   
    음수 영역에서 지수적 감소를 통해 0에 가까워지게 하여 노이즈에 강건하게 만든다.  
    **Maxout:**   
    $max(w_1^Tx+b_1, w_2^Tx+b_2)$ 형태로 ReLU와 Leaky ReLU를 일반화하지만 파라미터 수가 2배가 된다.

일반적으로는 **ReLU**를 가장 먼저 시도하고 학습률 등을 주의 깊게 설정해야 한다. 시그모이드는 사용하지 않는 것이 좋다.

### 3. Data Preprocessing
입력 데이터를 신경망에 넣기 전에 처리하는 과정이다.

**Zero-centering (평균 제거):**   
데이터의 평균을 0으로 만든다. 앞서 시그모이드의 문제점처럼 입력이 모두 양수일 때 그레디언트가 한 방향으로만 움직이는 문제를 방지하기 위해 필수적이다.

**Normalization (정규화):**   
데이터의 스케일을 맞추는 과정이지만, 이미지 데이터의 경우 픽셀들이 이미 비슷한 스케일(0~255)을 가지므로 일반적으로 수행하지 않거나 덜 중요하다.

**PCA/Whitening:**   
데이터의 차원을 줄이거나 상관관계를 없애는 기법이지만 이미지 처리(CNN)에서는 공간적 구조를 유지해야 하므로 잘 사용하지 않는다.

**테스트 단계:**   
전처리에 사용하는 평균값은 오직 **Training Data**에서만 계산하며, 테스트 데이터에도 학습 데이터에서 구한 평균값을 동일하게 적용하여 뺍니다. CNN에서는 이미지 전체 평균을 빼거나 채널별(R, G, B) 평균을 빼기도 한다.

### 4. Weight Initialization
학습 시작 전 가중치 $W$를 어떻게 설정하느냐가 학습의 성공 여부를 결정한다.

**모두 0으로 초기화 (All Zero):**   
모든 뉴런이 같은 연산을 하고 같은 그레디언트를 받게 되어 결국 모든 뉴런이 똑같이 변하게 된다(Symmetry breaking 실패). 절대 사용하면 안 된다.

**작은 난수 초기화 (Small Random Numbers):**   
$0.01 \times Gaussian$ 등으로 초기화하면 얕은 신경망에서는 작동하지만 깊은 신경망에서는 층을 거칠수록 Activation이 0으로 수렴하게 된다. 이 경우 역전파 시 그레디언트도 0이 되어 학습이 되지 않는다.

**큰 난수 초기화 (Large Random Numbers):**   
가중치를 크게 하면 Tanh 등의 활성화 함수가 포화(Saturate)되어 출력이 -1 또는 1이 되고, 그레디언트는 0이 되어 학습이 멈춘다.

**Xavier Initialization (Glorot):**   
입출력의 분산(Variance)을 유지하기 위해 가중치를 $\text{Standard Gaussian} / \sqrt{\text{입력 개수}}$로 스케일링한다. Tanh 등을 사용할 때 적합하다.

**He Initialization:**   
ReLU를 사용할 때는 입력의 절반이 0이 되어 분산이 절반으로 줄어들기 때문에, Xavier 초기화 값에 2를 곱해주는 보정이 필요하다($/ \sqrt{\text{입력 개수}/2}$). ReLU 사용 시 가장 권장되는 방식이다.

### 5. Batch Normalization
가중치 초기화에만 의존하지 않고 학습 과정 내내 각 레이어의 입력이 강제로 정규 분포를 따르도록 만드는 기법이다.

**작동 원리:**   
각 Mini-batch마다 평균과 분산을 계산하여 Normalize한다. 이는 미분 가능한 연산이므로 역전파가 가능하다.

**Scale and Shift ($\gamma, \beta$):**   
정규화 후에 다시 스케일을 조정($\gamma$)하고 이동($\beta$)시키는 파라미터를 추가하여 학습시킨다. 이는 네트워크가 원한다면 정규화를 풀고 원래의 분포(Identity)나 다른 분포를 표현할 수 있게 유연성을 준다.

**CNN에서의 적용:**   
공간적 구조를 유지하기 위해, 각 채널(Activation Map) 단위로 통계를 내어 정규화한다.

**장점:**   
그레디언트 흐름이 좋아져 학습 속도가 빨라지고, 초기화나 Learning Rate 설정에 덜 민감해지며, 약간의 Regularization 효과도 있다.

**테스트 단계:**   
테스트 시에는 배치의 평균을 쓸 수 없으므로, 학습 중에 계산해둔 이동 평균을 사용하여 정규화한다.

### 6. 학습 과정 관리 
실제 모델을 학습시킬 때 단계별 점검 사항이다.

1.  **데이터 전처리 & 아키텍처 선택:**   
데이터를 0-평균으로 만들고 모델 구조를 정한다.
2.  **Sanity Check (초기 손실 확인):**   
규제(Regularization) 없이 초기 손실값이 예상치(예: 10개 클래스면 $-\log(0.1) \approx 2.3$)와 맞는지 확인한다. 그 후 규제를 넣었을 때 손실값이 증가하는지 확인한다.
3.  **작은 데이터로 과적합(Overfit) 시키기:**   
아주 적은 양의 데이터로 학습시켜 손실이 0에 가깝게 떨어지고 정확도가 100%가 되는지 확인한다. 이는 모델과 구현에 버그가 없음을 검증하는 단계이다.
4.  **전체 학습 및 학습률(Learning Rate) 탐색:**  

    학습률이 너무 낮으면 손실이 거의 변하지 않는다.
    학습률이 너무 높으면 손실이 폭발(NaN)한다.
    대략 $10^{-3} \sim 10^{-5}$ 범위에서 적절한 값을 찾다.

### 7. Hyperparameter Optimization
**Cross Validation:**   
훈련 세트로 학습하고 검증 세트로 평가한다.

**탐색 전략:**   
처음에는 넓은 범위에서 적은 Epoch으로 **대략적인(Coarse)** 탐색을 하고, 좋은 범위가 발견되면 그 안에서 **정밀한(Fine)** 탐색을 수행한다.

**로그 스케일(Log Space):**   
학습률 등은 곱셈적으로 작용하므로 균등 분포가 아닌 로그 스케일($10^{-3}, 10^{-4}...$)로 탐색해야 한다.

**Random Search 권장:**   
Grid Search보다 무작위 탐색이 중요한 파라미터의 다양한 값을 더 잘 탐색할 수 있어 효율적이다.

**손실 곡선 및 정확도 모니터링:**  
    손실 곡선이 평평하다가 갑자기 떨어지면 초기화가 잘못된 것일 수 있다.  
    훈련 정확도와 검증 정확도의 Gap가 크면 Overfitting, 차이가 없으면 모델 Underfitting일 수 있다.  
    업데이트 크기와 가중치 크기의 비율을 약 0.001 정도로 유지하는 것이 좋다.

**ReLU 사용, 평균 빼기 전처리 수행, Xavier/He 초기화 사용, Batch Norm 사용, 그리고 하이퍼파라미터는 무작위 탐색으로 찾을 것**. 
