---
layout: post
title:  "[논문정리] Denoising Diffusion Probabilistic Models" 
date:   2025-11-17 09:50
categories: mystudy CV
tags: cv
use_math: true

---

# abstract
비평형 열역학에서 영감을 받은 변수 모델인 확산 확률 모델을 사용하여 고품질 이미지 합성 결과를 제시한다. 확산 확률 모델과 랑주뱅 동역학을 이용한 노이즈 제거 점수 매칭 간의 새로운 연관성을 기반으로 설계된 가중치 변분 경계를 사용하여 학습함으로써 최상의 결과를 얻울 수 있었다. 이 모델은 자기 회귀 디코딩의 일반화로 해석될 수 있는 점진적 손실 감압 방식을 자연스럽게 수용한다. <br>
CIFAR10 데이터셋에서 Inception 점수는 9.46, FID 점수는 3.17을 기록했다. 256X256 LSUN에서 Progressive GAN 과 유사한 샘플 품질을 얻었다. 


> Diffusion Probabilistic Models (Diffusion Models): 데이터에 점진적으로 가우시안 노이즈를 추가하는 순방향 과정(forward process)과 이 과정을 역전시키도록 학습된 마르코프 체인(Markov chain)을 사용하는 생성 모델
{: .prompt-info }

> 점진적 손실 감압 방식 (Progressive Lossy Compression): 
점진적으로 노이즈를 추가하여 정보를 손상시키는 과정과 이 손상된 정보로 원본 데이터를 복원하는 과정을 학습하는 방식
{: .prompt-info }

> 가중 변분 경계 (Weighted Variational Bound): 
목적함수. 실제 데이터의 복잡한 확률 분포를 직접 계산하는 대신 계산 가능한 하한선(Evidence lower bound, ELBO)을 설정하고 이를 최대화 하여 모델을 학습시킨다. 여기에 가중치를 부여하여 원본 복원 품질과 잠재 공간의 정규화 사이의 균형을 조절하는 방식을 가중변분 경계라고 한다. 
{: .prompt-info }

> 랑주뱅 동역학(Langevin Dynamics): 
물리학에서 입자의 무작위적 움직임을 설명하는 방정식. 머신러닝에서 이 원리를 샘플링 기법에 응용한다. 특정 확률 분포에서 데이터를 생성할 때 밀도가 높은 방향으로 이동시키면서 적절한 노이즈를 추가하는 과정을 반복하여 원하는 분포의 샘플을 얻는다. 
{: .prompt-info }

> 비평형 열역학 (Non-equilibrium Thermodynamics): 
시스템이 열역학적 평형(안정된) 상태에 있지 않을때 에너지 흐름을 다루는 물리학. DPM은 이 개념에 이론적 기반을 둔다. 
데이터를 평형상태로 노이즈를 비평형 상태로 보고 데이터가 노이즈로 변하는 과정과 노이즈에서 데이터를 보원하는 과정을 열역학적 흐름으로 모델링한다. 
{: .prompt-info }

> LSUN (Large-scale Scene Understanding): 
대규모 장면 이해를 위한 데이터셋이다. 다양한 장소와 카테고리별로 수배만장의 이미지를 제공한다.
{: .prompt-info }


# 1. Introduction

![alt](/assets/img/DDPMfig1.png)

![alt](/assets/img/DDPMfig2.png)

Diffusion model은 Markov chain을 매개변수화 해서 훈련시키는 방식이다. 데이터에 점점 노이즈를 추가하는 확산 과정을 역으로 학습해서 최종적으로는 원본 데이터와 일치하는 샘플을 생성한다. <br>
조금씩 노이즈를 추가하면서 데이터를 흐리게 만들고 다시 노이즈를 없애며 깨끗한 이미지를 복원한다. 

# 2. Background

Diffusion model은 다음 수식의 latent variable 모델이다.
$$p_θ(x_0)=:∫p_θ(x_{0:T} )d_{x1:T}$$

$x1, ⋯, x_T
는 데이터 
x_0~q(x_0)와 같은 크기이다. $

$결합 분포 p_θ(x_{0:T})는 reverse process이다. 
p(x_T)=N(x_T;0,I)
에서 시작하는 Gaussian transition으로 이루어진 Markov chain으로 정의된다.$

$$
p_\theta (x_{0:T}) := p(x_T) \prod_{t=1}^T p_\theta (x_{t-1}|x_{t}) \\
p_\theta (x_{t-1}|x_{t}) := \mathcal{N} (x_{t-1} ; \mu_\theta (x_t , t), \Sigma_\theta (x_t , t))
$$

Diffusion 모델이 다른 latent variable 모델들과 다른 점은 forward process, diffusion process라고 불리는 근사 posterior $ q(x_{1:T} ∣x_0 )$
가 고정된 마르코프 체인이다. forward, diffusion process는 variance schedule 
$ β_1,⋯,β_T$에 따라 가우시안 노이즈를 점진적으로 추가한다.
<br> 
정리하면 이미지에 노이즈를 추가하는 forward process는 신경망이 학습하는 것이 아니라 미리 정해둔 규칙으로 자동으로 진행되는 것이다. 


$$
\begin{aligned}
q (x_{1:T}|x_0) &:= \prod_{t=1}^T q (x_{t}|x_{t-1}) \\
q (x_{t}|x_{t-1}) &:= \mathcal{N} (x_{t} ; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
\end{aligned}
$$

학습은 negative log likelihood 의 variational bound를 최적화하는 것으로  진행된다. 
모델 파라미터 $θ$ 를 조정하면서 모델이 주어진 데이터 $x_0$ 에 대해 최선의 예측을 하도록 한다. 

$$
L:= \mathbb{E} [-\log p_\theta (x_0)] \le \mathbb{E}_q \bigg[ -\log \frac{p_\theta (x_{0:T})}{q(x_{1:T}|x_0)} \bigg]
\le \mathbb{E}_q \bigg[ -\log p(x_T) - \sum_{t \ge 1} \log \frac{p_\theta (x_{t-1}|x_t)}{q(x_t|x_{t-1})} \bigg]
$$




variance schedule
가우시안 노이즈
latent variable
log likelighood 
negative log likelihood
variantional bound



forward process의 분산 !! 베타 t !! 는 reparameterization 을 통해 학습하거나 하이퍼 파라미터로 설정할 수 있다. <br>
reverse process의 expressiveness는 !! p!! 에서 가우시안 conditionals를 선택해 부분적으로 보장된다. revese process에서 상태를 예측하는 과정이 수학적으로 잘 정의되고 계산이 간단해지며 모델의 표현력이 어느정도 보장된다. 





expressiveness