---
layout: post
title:  "idea" 
date:   2026-11-06 10:35
categories: mystudy 
tags: 
---


# ReLU
활성화함수가 비선형성을 추가하기 위함인데
렐루함수에서는 양수일때는 그대로 쓰는데 그게 어떻게 활성화함수로 역할을 하는지
> 선형적이라는 것이 잘 정의되어있지 않음. 그래서 0이하까지 같이봐서 그게 같이보면 비선형적으로 보이니 그렇다고 한다.

# 목적함수
(생각정리가 잘 안됨.)
목적함수 1/n하는데 저기 목적함수는 왜 1/c를 안하는지. 
그리고 1/2를 곱해도 왜 유효한지. 결과가 같은지


![alt](/assets/img/ida목적함수.png)





# cnn에서 Flatten 을 써야하나?

Flatten이 기본이라고는 하는데,
Flatten과 GAP(Global Average Pooling)의 구조적 차이, 파라미터 수, 과적합 측면에서 어떤 영향을 주는지 솔직히 정확히는 모르겠다.
왜 어떤 모델은 Flatten을 쓰고 어떤 모델은 GAP을 쓰는지 기준이 궁금함.

gap: 채널별로 평균값을 추출하여 피처맵을 생성하는 풀링
공간정보를 덜 잃는다고 하던데 평균으로 확 줄여버리는데 어떻게 공간 정보를 덜 잃는 것인지


# cnn dropout 0.4로 정했는데 적용 기준



# 8-2에 41페이지에 위치 정보를 담고 있다고 하셨는데
이게 특성맵이 담고있는 건지

특성맵이면 뒤가 flatten인데 의미있는 것인지
flatten이면 1차원으로 값이 나열된 것인데 어떻게 위치 정보가 있는 것인지


# 8-3에서 사진을 보면 엄청 비슷하게 생겼는데 어떻게 커널은 다르게 생겼는지 그 이유
두 모델 성능, 구성이 같더라도 채널은 완전 내용이 달라짐

llm 모델 병합
매칭 뭐랑 뭐랑 해야하는지
(?) 뭐라고 적었던 거지?


