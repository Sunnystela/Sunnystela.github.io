---
layout: post
title: VAE
date: 2026-01-15 12:50
categories: MyStudy 
tags: DeepLearning
math: true
---


>VAE는 잠재 변수 z가 표준 정규 분포를 따른다고 가정한다. z공간을 몰라서 정규 분포로 가정한다고 하는데 왜 하필 정규분포로 하는 것인가?
{: .prompt-tip }

![alt](/assets/img/vae.png)


**간단하고, 계산이 쉽고, 그럼에도 그럭저럭 잘 먹히는 기본값이기 때문**


## 1. 계산이 편하다


VAE는 $q_\phi(z\mid x)$와 $p(z)$ 둘 다 **정규분포**로 둔다. 

>왜 q까지 정규분포로 고정인지

이렇게 두면 ELBO의 KL term $D_{\text{KL}}(q_\phi(z\mid x)\parallel p(z))$이 **폐쇄형 해석식**으로 떨어져서 따로 Monte Carlo 추정 없이도 gradient를 바로 구할 수 있다. 

>ELBO 식 안에 위에 식이 들어가는데 이는 두 분포 사이의 거리를 숫자로 재는 함수다.  
두 분포가 모두 정규분포이면 식에서 닫힌 형태 식이 존재해서  
샘플링해서 추정(Monte Carlo estimate)할 필요 없이  
μ,σ에 대한 explicit formula로 KL을 계산할 수 있고  
그 식을 그대로 미분해서 gradient를 구할 수 있다.  
역전파에 부담을 주지 않는다



>Monte Carlo  
계산하기 어려운 값을 직접 식으로 풀지 않고 랜덤 샘플을 여러 번 뽑아서 평균을 내서 근사하는 방법  
{: .prompt-info }

>Monte Carlo & VAE  
일반적으로 
$E_q[log \frac{q(z)}{p(z)}]$ 같은 건 적분이라서 Monte Carlo로 z를 여러 개 뽑아서 평균 내서 근사할 수 있고  
하지만 두 분포가 정규분포라 KL에 대해 닫힌 형태 식이 있으니까  
Monte Carlo로 샘플링해서 근사할 필요 없이 공식으로 바로 계산할 수 있다



reparameterization trick이 깔끔해진다. 

>reparameterization trick  
확률적 sampling 과정이 포함된 딥러닝 모델에서 미분 불가능한 샘플링 과정을 미분 가능한 구조로 바꾸어 역전파 학습이 가능하게 만드는 기법
{: .prompt-info }


>샘플링 자체는 미분 불가능하지만 정규 분포를 쓰면 reparameterization trick이 단순해진다.  
ELBO의 reconstruction term을 Monte Carlo로 근사하고 
reparameterization trick으로 그 gradient를 계산한다.


$$z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$$

다른 prior(heavy-tailed, mixture 등)를 쓰면 KL이 분석적이지 않거나, reparameterization이 복잡해져서 구현, 최적화 난이도가 올라간다. 

>분석적이지 않다  
KL divergence 값을 계산할 때 닫힌 형태의 정확한 공식이 없어서 수치적 근사(Monte Carlo 샘플링 등)를 써야 한다 -> 느리고 불안정

## 2. 좋은 초기 가설 + 하이퍼가 없는 기본값

$N(0,I)$는  
  non-trainable,
  파라미터가 거의 없고,  
  구현이 한 줄짜리인 **가장 단순한 continuous prior**다. 

>p(z) = N(0,I)
→ μ=0, σ=1 고정 → 파라미터 없음!


latent의 각 차원을 독립, 동일 분포로 보는 가정은 뭐가 진짜인지 모르겠으니, 일단 feature들이 비슷한 스케일, 역할을 한다는 **중립적인 초기 가설** 역할을 한다. 

>latent 차원들 간 가정:  
N(0,I) → 모든 차원 독립, 같은 스케일
     → 뭐가 중요한지는 모르겠으니 다 똑같다 가정  



중립적 초기 가설 덕분에 모델 복잡도를 latent 쪽이 아니라 encoder, decoder 아키텍처와 likelihood 쪽에 집중할 수 있다. 

>현실 데이터:  
feature들이 서로 다른 스케일, 중요도 가짐
→ encoder가 이걸 자연스럽게 배움

>encoder가 다른 스케일, 중요도를 배움이 가능한가?  
중요도 높은 차원  
   σ↓ μ↑ → KL 작음 → 자유롭게 사용  
덜 중요 차원    
   σ↑ μ↓ → KL 큼 → 제약됨



연구 초반에는 prior 자체도 튜닝해야 한다보다 일단 잘 알려진 기본값 위에서 나머지를 바꿔보자가 훨씬 다루기 쉽다. 


## 3. 매끄러운 latent 공간과 샘플링


KL term이 $q_\phi(z\mid x)$를 ${N}(0,I)$ 근처로 끌어당기면서 데이터 전체를 하나의 **연속적인 공통 공간**에 embed하게 만든다. 

그 결과:  
  latent interpolation이 꽤 자연스럽고,  
  학습 후 $z\sim{N}(0,I)$에서 샘플링하면 “어디에서 뽑아도 그럴듯한 샘플”이 나오는 경향이 있다.

prior가 multimodal이나 weird하면 어디에서 샘플링해야 안전한지 latent 거리가 의미 있는지 등이 훨씬 애매해진다. 

정규 prior는 latent space를 비교적 매끄럽게 만들고 샘플링 전략을 단순하게 해 주는 기본 설계다. 

## 4. 그렇다고 정규분포가 정답은 아니다 


표준 정규 prior는 **너무 단순해서 over-regularization**을 일으키고 latent가 충분히 정보를 못 담는 문제가 있다는 것도 많이 보고됐다.

최적 prior는 사실상 aggregated posterior $p(z)=\frac{1}{N}\sum_x q_\phi(z\mid x)$ 인데, 이건 계산이 불가능해서 VampPrior 같은 근사 기법이 연구되고 있다. 
//확인

heavy-tailed prior(예: Student‑t), conditional prior, hierarchical prior 등을 쓰면 multimodal 데이터·복잡한 구조에서 더 좋은 표현을 얻는다는 결과들도 계속 나오고 있다. 

아무 prior나 말고 정규분포를 쓰는 이유는  
수학적으로 다루기 쉽고, 구현이 단순하면서, latent를 reasonable하게 regularize해 주는 **강력한 baseline**이기 때문이지, 이게 항상 최적이라서가 아니다.







## 참고자료

1. [딥러닝 Ch3.3 VAE](https://www.youtube.com/watch?v=GbCAwVVKaHY)

2. [변분형 자동 인코더](https://www.youtube.com/watch?v=qJeaCHQ1k2w)

