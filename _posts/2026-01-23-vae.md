---
layout: post
title: "VAE"
date: 2026-01-15 12:50
categories: MyStudy DeepLearning.AI
tags: DeepLearning.AI DeepLearning
math: true
---


>VAE는 잠재 변수 z가 표준 정규 분포를 따른다고 가정한다. z공간을 몰라서 정규 분포로 가정한다고 하는데 왜 하필 정규분포로 하는 것인가?
{: .prompt-tip }

![alt](/assets/img/vae.png)


**간단하고, 계산이 쉽고, 그럼에도 그럭저럭 잘 먹히는 기본값이기 때문**


## 1. 계산이 편하다


vanilla VAE는 $q_\phi(z\mid x)$와 $p(z)$ 둘 다 **정규분포**로 둔다. 

이렇게 두면 ELBO의 KL term $\mathcal{D}_{\text{KL}}(q_\phi(z\mid x)\parallel p(z))$이 **폐쇄형 해석식**으로 떨어져서 따로 Monte Carlo 추정 없이도 gradient를 바로 구할 수 있다. 

reparameterization trick도 깔끔해진다. 

$$z = \mu_\phi(x) + \sigma_\phi(x)\,\epsilon,\ \epsilon\sim\mathcal{N}(0,I)$$

다른 prior(heavy-tailed, mixture 등)를 쓰면 KL이 analytic하지 않거나, reparameterization이 복잡해져서 구현·최적화 난이도가 확 올라간다. 


## 2. 좋은 초기 가설 + 하이퍼가 없는 기본값


\(\mathcal{N}(0,I)\)는  
  
  non-trainable,  
  
  파라미터가 거의 없고,  
  
  구현이 한 줄짜리인 **가장 단순한 continuous prior**다. [jmtomczak.github](https://jmtomczak.github.io/blog/7/7_priors.html)

latent의 각 차원을 독립·동일 분포로 보는 가정은 “뭐가 진짜인지 모르겠으니, 일단 feature들이 비슷한 스케일/역할을 한다”라는 **중립적인 초기 가설** 역할을 한다. [rbcborealis](https://rbcborealis.com/research-blogs/tutorial-5-variational-auto-encoders/)

이 덕분에 모델 복잡도를 latent 쪽이 아니라 encoder/decoder 아키텍처와 likelihood 쪽에 집중할 수 있다. [arxiv](https://arxiv.org/pdf/1910.02760.pdf)

연구·실무 초반에는 “prior 자체도 튜닝해야 한다”보다 “일단 잘 알려진 기본값 위에서 나머지를 바꿔보자”가 훨씬 다루기 쉽다. [jmtomczak.github](https://jmtomczak.github.io/blog/7/7_priors.html)


## 3. 표현·geometry 관점: 매끄러운 latent 공간과 샘플링


KL term이 \(q_\phi(z\mid x)\)를 \(\mathcal{N}(0,I)\) 근처로 끌어당기면서, 데이터 전체를 하나의 **연속적인 공통 공간**에 embed하게 만든다. [papers.neurips](http://papers.neurips.cc/paper/8553-learning-hierarchical-priors-in-vaes.pdf)

그 결과:  
  
  latent interpolation이 꽤 자연스럽고,  
  
  학습 후 \(z\sim\mathcal{N}(0,I)\)에서 샘플링하면 “어디에서 뽑아도 그럴듯한 샘플”이 나오는 경향이 있다. [ecai2020](https://ecai2020.eu/papers/1330_paper.pdf)

prior가 multimodal이나 weird하면, 어디에서 샘플링해야 안전한지, latent 거리가 의미 있는지 등이 훨씬 애매해진다. [papers.neurips](http://papers.neurips.cc/paper/8553-learning-hierarchical-priors-in-vaes.pdf)

즉, 정규 prior는 “latent space를 비교적 매끄럽게 만들고, 샘플링 전략을 단순하게 해 주는” 기본 설계다. [ecai2020](https://ecai2020.eu/papers/1330_paper.pdf)


## 4. 그렇다고 정규분포가 정답은 아니다 (연구 포인트)


표준 정규 prior는 **너무 단순해서 over-regularization**을 일으키고, latent가 충분히 정보를 못 담는 문제가 있다는 것도 많이 보고됐다. [proceedings.mlr](http://proceedings.mlr.press/v84/tomczak18a/tomczak18a.pdf)

“최적 prior”는 사실상 aggregated posterior \(p^\*(z)=\frac{1}{N}\sum_x q_\phi(z\mid x)\)인데, 이건 계산이 불가능해서 VampPrior 같은 근사 기법이 연구되고 있다. [arxiv](https://arxiv.org/html/2412.01373v1)

heavy-tailed prior(예: Student‑t), conditional prior, hierarchical prior 등을 쓰면 multimodal 데이터·복잡한 구조에서 더 좋은 표현을 얻는다는 결과들도 계속 나오고 있다. [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S0031320323002303)

그래서 조금 세련되게 말하면:

> 아무 prior나 말고 정규분포를 쓰는 이유는,  
>  “수학적으로 다루기 쉽고, 구현이 단순하면서, latent를 reasonable하게 regularize해 주는 **강력한 baseline**이기 때문이지, 이게 항상 최적이라서가 아니다.” [pure.uva](https://pure.uva.nl/ws/files/42635680/tomczak18a.pdf)








## 참고자료

1. [딥러닝 Ch3.3 VAE](https://www.youtube.com/watch?v=GbCAwVVKaHY)

2. [변분형 자동 인코더](https://www.youtube.com/watch?v=qJeaCHQ1k2w)

